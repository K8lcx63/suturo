\documentclass{suturo}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage{spverbatim}
\usepackage{caption}
\usepackage{graphicx}
\usepackage{tabto}
\makeatletter
\newcommand{\chapterauthor}[1]{%
  {\parindent0pt\vspace*{-27pt}%
  \linespread{0}\small\begin{flushright}von: #1\end{flushright}%
  \par\nobreak\vspace*{0pt}}
  \@afterheading%
}
\makeatother

\begin{document}
\begin{titlepage}
	\centering
	{\scshape\LARGE Sudo tidy up my room \glqq Suturo\grqq{} \par}
	\vspace{1.5cm}
	{\scshape\Large Meilenstein 3\par}
	\vspace{1.5cm}
	{\huge\bfseries Projektbericht \par}
	\vspace{2.5cm}
	{\normalsize\bfseries Verfasst von \par}
	{\small\itshape Roman Haak, Maximilliam Bertram\par}
	{\small\itshape Alexander Link, Tammo W\"ubbena\par}
	{\small\itshape Vanessa Hassouna, Kevin St\"ormer, Hauke Tietjen\par}	
	{\small\itshape Alexander Haar \&  Max-Phillip Bahr\par}	
	\vspace{2.5cm}
	{\normalsize\bfseries Tutoren: \par}
	{\small\itshape Georg Bartels, Ferenc Balint-Benczedi \\ Daniel Be{\ss}ler \& Gayane Kazhoyan\par}
	\vfill
\end{titlepage}

\tableofcontents

\newpage

\section*{Einleitung}
\section{Vorwort}
\chapterauthor{Kevin Störmer, Maximilian Bertram}
Im Rahmen unserer Bachelorprojektarbeit 'Suturo', haben wir die Chance ein intelligentes Robotersystem, am Roboter 'PR2' von Willow Garage zu entwickeln. Das Projekt hat sich insgesamt in drei Meilensteine gegliedert, die iterativ aufeinander aufbauen. Der Code wurde in jedem Meilenstein weiterentwickelt und es wurden neue Ziele auf Basis der erreichten Erfolge des letzten Meilensteins definiert.
In den folgenden Abschnitten wollen wir allem voran einleitend auf planende Aspekte, sowie unsere Vorgehensweise und Zielsetzung eingehen. Daraufhin soll im Abschnitt zu unserer Schnittstellendokumentation, auf konkrete Lösungen der einzelnen Projektgruppen eingegangen werden.\\ ~ \\
Abschliessend finden sie eine Installationsanleitung im letzten Abschnitt unserer Dokumentation.



\newpage

\section{Zielsetzung}
\subsection{Allgemeine Zielsetzung}
\chapterauthor{Maximilian Bertram}
Unser Ziel war es zu erst eine Basis zu schaffen, auf der wir iterativ aufbauen konnten. Dazu war es nötig Nodes
für die einzelnen Teilbereich, die für ein intelligentes Gesamtsystem notwendig sind zu schaffen. Hier zu zählt
die Erkennung von Objekten, das kollisionsfreie Planen von Bewegungen, die Wissensrepräsentation über alle für den Gesamtablauf relevanten Fakten, sowie einem Plan, der den Ablauf skizziert und auf eventuelle Fehler reagieren kann. Das Ziel war es den Roboter intelligent Objekte aufräumen zu lassen und diese vom Arbeitsbereich der Küche auf vordefinierte Bereiche der anderen Küchenzeile zu stellen. Hier zu haben wir einige Prämissen getroffen, sowie Anforderungen entwickelt um den Gesamtablauf möglichst intelligent und reibungslos zu gestalten.

\subsection{Prämissen}
\chapterauthor{Kevin Störmer, Maximilian Bertram}
\begin{itemize}
\item Der Pr2 befindet sich beliebig im Raum zwischen Küche und Küchenzeile.
\item Auf der Küchenzeile befindet sich eine beliebige Anzahl Gegenstände aus einer festgelegten Menge (Objektliste im Anhang.
\item Es befinden sich keine Fremdkörper auf dem Boden der Küche, oder im Arbeitsbereich des PR2.
\item Der Platz, an dem der PR2 die Objekte abstellen soll ist frei oder nur mit Objekten, die der PR2 kennt bestückt.
\end{itemize}

\subsection{Anforderungen}
\chapterauthor{Maximilian Bertram}
Um den Gesamtablauf besser zu definieren wurden folgende Anforderungen an den Ablauf gestellt:
\begin{itemize}
\item Es ist ein Set an Objekten bekannt, es spielt keine Rolle, welche der bekannten Objekte auf der Küchenzeile stehen.
\item Der PR2 kennt verschiedene Greifposen und wählt die beste der ihm bekannten Greifposen aus, so dass es beim Greifen keine Kollisionen mit der Küche oder anderen Objekten gibt.
\item Ist das Greifen mit obigen Einschränkungen nicht möglich, verändert der PR2 seine Position um das Objekt in einem anderen Winkel greifen zu können.
\item Falls das Greifen immer noch nicht möglich ist, wird der Mensch um Hilfe gebeten um den Gesamtablauf nicht abbrechen zu müssen.
\item Die Kraft, die zum Greifen genutzt wird, ist pro Objekt definierbar und so eingestellt, dass die Objekte nicht beschädigt werden.
\item Es werden beide Gripper genutzt um den Gesamtablauf zu beschleunigen.
\item Der PR2 ordnet die Objekte in verschiedene Kategorien um die Fahrwege zu optimieren.
\item Es kommt zu keinen Kollisionen beim Fahren durch die Küche.
\item Ist der Abstellplatz für das Objekt belegt, wird versucht das Objekt, welches sich auf dem gewünschten Abstellplatz befindet zu verschieben um so Platz für das neue Objekt zu schaffen.
\item Die Klassifizierung der Objekte soll keinen unnötigen Traffic im Netzwerk erzeugen.
\end{itemize}


\subsection{Ablauf}
\textbf{-----------------!!!!TODO VERWEIS!!!!-----------------------}


\subsection{Abschluss}
\begin{itemize}
\item Die Küchenzeile ist leer.
\item Alle Objekte, welche ehemalig auf der Küchenzeile standen, befinden sich in den vorgesehenen Bereichen.
\item Der Pr2 befindet sich in der Home-Position
\end{itemize}

\section{Aufgabenverteilung}
\chapterauthor{Störmer, Hassouna, Bahr, Bertram, Haar}
Die Aufgabe der Gruppe Planning, ist zum einen die Etablierung einer Kommunikationsschnittstelle zwischen den Funktionalitäten der anderen Gruppen, zum anderen die Entwicklung eigener Funktionalitäten. Die Gruppe möchte hier besonderen Wert auf die Navigation des Roboters in der Welt, Sicherheitskritischem Errorhandling und der Identifikation von Ablageorten für jedes bekannte Objekt. \\

Das Ziel der Gruppe Vision soll es sein, mehrere Objekte in einer wahrgenommenen Szene zu erkennen und zu segmentieren .Diese Objekte sollen extrahiert werden und die Posen der einzelnen Cluster zur weiteren Verarbeitung übergeben werden. \\

Die Gruppe Knowledge ist zuständig für das Klassifizieren und Abspeichern von erkannten Gegenständen im Beliefstate, sowie das Bestimmen einer geeigneten GraspPose für jeden Gegenstand.\\

Das Ziel des Teams Motion ist es einen Actionserver zu implementieren, der verschiedene Aktionen für das Greifen, Abstellen und Umstoßen von Objekten anbietet. Jede der Aktionen soll mit beiden \textit{Grippern} möglich sein, dabei soll der Roboter Objekte in seiner Umgebung berücksichtigen und seine Bewegungspfade so planen, dass beim Bewegen keine Kollisionen entstehen. Ist eine kollisionsfreie Bewegung nicht möglich wird die Aktion mit einem entsprechenden Fehler abgebrochen.\\

\section{Herausforderungen}
\subsection{Routen können nicht richtig geplant werden}
\chapterauthor{Vanessa Hassouna}
Beim Fahren kollidiert der Roboter häufig mit Küchenobjekten und dreht sich in einem Winkel, der problematisch für seinen verletzbaren Rücken sein kann.

\textbf{Lösung}: Bevor der Pr2 zu dem gegebenem Punkt fährt, stellen wir sicher, dass sich der Roboter in einer entsprechenden sicheren Position befindet. Bis dahin sollte er sich um 90 Grad gedreht haben.

\subsection{Extrahieren von wichtigen Information}
\chapterauthor{Vanessa Hassouna}

Die Informationen der gesehenen Objekte wird als \textbf{Message} übergeben. Diese beinhaltet ein Array aus \textbf{normal\_features}, ein Array aus \textbf{color\_features}, ein Array mit \textbf{object\_pose} und die Anzahl der gesehenen Objekte. 
Um jedoch Objekte an Knowledge senden zu können, muss das Array zuerst zergliedert werden. \\

\textbf{Lösung}: Alle erhaltenen Informationen werden auf dem Parameterserver zwischengespeichert, erst danach werden diese miteinander konkateniert.

\subsection{Logik der Main wird zu groß und unübersichtlich}
\chapterauthor{Vanessa Hassouna}
Für eine sinnvolle Logik der Main ist eine gute Übersicht zu schaffen. Da die Logik der Main jedoch immer größer und dadurch unübersichtlich wird, passieren hier schnell Fehler.\\

\textbf{Lösung}: Die Logik der Main wurde in mehrere Teilabschnitte getrennt und in neue Funktionen geschachtelt. Eine weitere Schnittstelle \textbf{planning\_logic} dient zur Extraktion von logischen Zwischenabschnitten.


\subsection{Ein Gripper ist unerwartet leer}
\chapterauthor{Kevin Störmer}
Greift der Pr2 daneben, oder verliert er während der Fahrt ein Objekt aus dem Gripper, so könnte der Pr2 ein dieses überfahren oder fälschlicherweise ein Objekt ablegen, welches sich nicht in seinem Gripper befindet.\\

\textbf{Lösung}: Algorithmus stellt zur Laufzeit anhand der Joints des Gripper fest, ob diese Gripper gefüllt sind, oder nicht. Es können so beliebig im Code kritische Abschnitte definiert werden, in denen eine Laufzeitkontrolle notwendig ist. Sollte nun ein Gripper fälschlicherweise leer sein, kann eine Notfallroutine etabliert werden.

\subsection{Die Objektplatzierung soll dynamisch sein}
\chapterauthor{Hauke Tietjen}
Wenn jedes Objekt einen statischen Abstellplatz hätte, könnten identische Objekte miteinander kollidieren. Außerdem bringt es Abwechslung in den Programmablauf, wodurch Probleme aufgedeckt werden können. Ein Beispiel wäre Überlappungen von Objekten, in der Wahrnehmung. \\

\textbf{Lösung}: Objekte werden in ihren festgelegten Abstellzonen kollisionsfrei platziert, bis diese keinen Platz mehr bieten oder bei drohender Überfüllung ein Fehler geworfen werden kann.


\subsection{Fehler sollen hierarchisch sein}
\chapterauthor{Hauke Tietjen}
Man sollte feststellen können in welchen Bereichen Fehler auftreten und diese, aufgrund dessen, anders zu behandeln. Besonders in höher gelegenen Funktionen kann dies nützlich bei der Fehlerbehandlung sein. \\

\textbf{Lösung}: In einer Fehlerhierarchie können Fehler von anderen Fehlern erben. Daher eignet sie sich gut um über- und untergeordneten Programmabschnitten Fehler zuzuordnen.

\subsection{Bestimmen der GraspPose vom Objekt}
\chapterauthor{Max-Phillip Bahr}
Jedes Objekt muss mindestens eine GraspPose besitzen, an der es vom Roboter gegriffen werden kann.\\

\textbf{Lösung}: In unserer OWL-Ontologie haben wir für jedes Objekt manuell eine GraspPose eingespeichert. Diese wird dann von einer Node ausgelesen und weitergegeben.

\subsection{Segmentieren \& Extrahieren von mehreren Objekten}
\chapterauthor{Tammo Wübbena}
Die Objekte müssen klar von der restlichen wahrgenommenen Szene extrahiert und in einzelne Cluster aufgeteilt werden.\\
\textbf{Lösung}: Die Planare werden so lang segmentiert, bis nur noch eine Fläche (der Tisch) übrig ist. Es werden dann nur die Punkte oberhalb der Tischfläche extrahiert. Dann werden die Objekte mit einer euclidean cluster extraction voneinander getrennt und als einzelne Objekte zwischengespeichert.

\subsection{Features zur Klassifizierung ermitteln}
\chapterauthor{Tammo Wübbena}
Normal- und Farb-Features werden zur Klassifizierung der ermittelten Objekte benötigt. \\
\textbf{Lösung}: Bestimmung von VFH-Features und aufteilen von RGB-Daten in Achter-Bins.

\subsection{Posen der Objekte bestimmen}
\chapterauthor{Tammo Wübbena}
Für eine sinnvolle Greifpose müssen die Punket im Koordinatensystem sowie Rotation und Translation der Objekte bestimmt werden. 
\textbf{Lösung}: Vision erhält von Knowledge die klassifizierten Labels und Nummern der Objekte, deren Pose dann mit einem Alignment-Algorithmus und einer Verfeinerung durch den Iterative Closest Point ermittelt werden.

\subsection{Welche Gripper sind belegt}
\chapterauthor{Alexander Haar}
Um Objekte zu greifen muss bekannt sein, ob sich im Gripper bereits ein Objekt befindet.\\
\textbf{Lösung}: Um herauszufinden, welcher Gripper gerade belegt ist wurde ein aktionsbasierter beliefstate verwendet. Ist für ein Objekt die zuletzt ausgeführte Aktion eine Greifaktion, dann muss sich das Objekt in dem angegeben Gripper befinden, also ist der Gripper belegt.

\subsection{Welche Objekte sollen weggeräumt werden}
\chapterauthor{Alexander Haar} 
Um herauszufinden, welche Objekte als nächstes weggeräumt werden sollen muss klar sein, welche Objekte sich noch auf der Kücheninsel befinden.\\
\textbf{Lösung}: Hier ist wieder der aktionsbasierte beliefstate das beste Mittel. Alle Objekte, welche nur wahrgenommen wurden und noch nicht gegriffen wurden müssen sich noch auf der Kücheninsel befinden. Mithilfe des storage\_place- Pakets werden dann die Objekte ermittelt, welche als nächstes weggeräumt werden sollen.

\subsection{Kollisionsfreiheit beim Greifen eines Objektes}
\chapterauthor{Roman Haak}
Das kollisionsfreie Greifen von Objekten mit dem moveit-Framework lief leider nicht immer so, wie erwartet. Obwohl die Objekte der Küche und wahrgenommene Objekte korrekt zur Planningscene und Kollisionsmatrix hinzugefügt wurden, entstanden manchmal Kollisionen beim Ausführen von Bewegungen aus uns unersichtlichen Gründen. Die Kollision mit dem zu greifenden Objekt wurde von dem genutzten Bewegunsplaner immer erfolgreich verhindert. Die Kollision mit den Objekte der Küche jedoch nicht immer.\\
\textbf{Lösung}: Leider haben wir keine Lösung für dieses Problem gefunden und gehen davon aus, dass es ein Problem des genutzten Frameworks ist. Bei dem moveit-Framework traten an einigen Stellen Probleme auf, zu denen es des öfteren auch im moveit-Repository erstellte 'issues' gab, die aber bislang noch nicht behoben wurden. So blieb es teilweise Glückssache, ob bei einer ausgeführten Bewegung ein kollisionsfreier Weg genutzt wurde oder nicht.

\subsection{Unterschiede Simulation echter PR2}
\chapterauthor{Roman Haak}
Teilweise wurden Lösungen implementiert, die in der Simulation sehr gut funktionierten. Aber als sie dann auf dem echten Roboter ausprobiert wurden,
funktionierten sie nicht mehr wie erwartet.\\
\textbf{Lösung}: Das Problem lag im Endeffekt darin, dass die Basis des echten Roboters meist hochgefahren war, die Basis in der Simulation jedoch im initialen Zustand nicht. Abhilfe hat eine implementierte Klasse geschaffen, die die Basis des Roboters in der Simulation hochfährt.

\subsection{'Deterministisches' Umstoßen eines Objektes}
\chapterauthor{Roman Haak}
Zunächst wurde das Umstoßen eines Objektes einfach dadurch realisiert, dass der Arm zu einem Punkt zwischen Roboter und Objekt gefahren wurde und danach zu dem Objekt. Dabei war es aber immer unterschiedlich, welchen Weg \textit{MoveIt} für die zweite Bewegung des Armes, also das Umstoßen, berechnet und ausgeführt hat.\\
\textbf{Lösung}: Die Funktion wurde so umgeschrieben, dass erst zu einem 'Zwischenpunkt' gefahren wird. Dann wird von diesem 'Zwischenpunkt' bis zu dem Punkt, an den der Gripper als zweites gefahren werden soll, eine 10-Punkte-Trajektorie berechnet. Diese Trajektorie beschreibt dann den geraden Weg von dem 'Zwischenpunkt' zum 'Umstoßpunkt' am Objekt. Und somit ist sichergestellt, dass immer von vorne umgestoßen wird.

\subsection{Verwalten der Planningscene}
\chapterauthor{Roman Haak}
Dadurch, dass das Greifen nun kollisionsfrei und nicht mehr 'aus sicherer Höhe' implementiert wurde, kam das Verwalten der Planningscene des moveit-Frameworks als neue Herausforderung dazu. Hier kam es hin und wieder zu Problemen, wie z.B., dass sobald ein Objekt gegriffen wurde und es an den Gripper angefügt wurde, die weitere Bewegung des Roboters durch den Bewegungsplaner verhindert wurde, weil sich der Startzustand in Kollision befand (durch eine Kollision des Objektes mit dem Tisch auf dem es stand).\\
\textbf{Lösung}: Funktionen, die die Kollision zwischen gegriffenem Objekt und dem Tisch kurzzeitig erlaubten, bis das Objekt aus der Kollision 'herausgehoben' wurde, haben Abhilfe geschaffen.

\subsection{Filtern und Selektieren der Greifposen}
\chapterauthor{Roman Haak}
Es wurden uns zum Greifen eines Objektes mehrere mögliche Greifposen zur Verfügung gestellt. Die Greifposen, die nicht erreichbar sind herauszufiltern und die übrig gebliebenen Greifposen nach bestimmten Kriterien zu ordnen, war eine neue Herausforderung.\\
\textbf{Lösung}: Ob es kinematische Lösungen für das Erreichen einer bestimmten Pose gibt, wurde mit Hilfe eines Services berechnet. Das Ordnen der verbliebenen Posen wurde nach den zwei Kriterien 'Distanz des aktuellen und des gewünschten Zustandes' und 'Distanz zur nächsten Kollision des gewünschten Zustandes' realisiert.

\subsection{Abstellen mit Drucksensor}
\chapterauthor{Roman Haak}
Das Abstellen mit dem Drucksensor funktionierte leider nicht, weil die gegriffenen Objekte aus dem Gripper rutschten und der Kraft-Drehmoment-Sensor des Roboters so keinen ausreichenden Druck detektierte. Leider fehlte am Ende die Zeit dieses Problem noch zu lösen.

\subsection{Tests \& Validierung}
\chapterauthor{Maximilian Bertram}
Es gab nach größeren Änderungen am Code immer wieder Fehler der Funktionalität der Nodes, sowie build Probleme.
Fehler in der Funktionalität sind leider erst beim testen auf dem Roboter aufgefallen, hier durch hatten wir eine extreme lange \textit{Time-To-Fail}, dies verlängerte die Entwicklungszeit unnötig und hinderte uns daran in dieser Zeit neue Features zu implementieren.
\textbf{Lösung}: Es werden Tests für die Funktionen des Codes geschrieben, die beim \textit{Bauen} des Projektes durchlaufen und den Code auf die korrekte Funktionalität prüfen. So sollen Fehler die bei Änderungen entstehen können schneller entdeckt werden und die \textit{Time-To-Fail} verkürzt werden.

\section*{Methodendokumentation Gruppe Planning}
\section{Die Quellcode-Datei: main.lisp (planning\_main\_programm)}
\subsection{Architekturbild}
\chapterauthor{Kevin Störmer}


\begin{figure}[!htb]
        \center{\includegraphics[width=0.4\textwidth]
        {img/diag_planning_main_programm.png}
        \caption{} Architektur der Quellcode-Datei main.lisp}
\end{figure}



\subsection{Beschreibung des Teilsystems}
\subsubsection{\"Ubersicht}
\chapterauthor{Kevin Störmer}
Die Quellcode-Datei 'main' im Paket 'planning\_main\_programm' soll ausschliesslich auf höchster Ebene den internen Ablauf des PR2 modellieren. 

Alle dafür notwendigen Logiken, die auf Basis externer Daten entschieden werden, wurden dabei basierend auf der Quelle der Daten in andere Quellcode-Dateien ausgelagert. Dabei wurde z.b der Vergleich zweier Punkte aus Vision in die Quellcode-Datei 'vision-communicate' (Paket: planning\_vision) ausgelagert. \\


Git-tag: Milestone 2:

\url{https://github.com/menanuni/planning_suturo_1718/releases/tag/0.2} \\

Git-tag Milestone 2 Branch TestObjectsPositioning aufgrund von merge Konflikten:

\url{https://github.com/menanuni/planning_suturo_1718/releases/tag/0.19}\\

\subsubsection{main ()}
\chapterauthor{Vanessa Hassouna}
\begin{verbatim}
main()
Die Main startet eine 'ros-node' mit dem Namen planning\_main.
Diese Node läuft solange,bis die komplette Main einmalig durchlaufen wurde. 
@return: T oder Nil 
\end{verbatim}



\subsection{Programmablauf}
\chapterauthor{Vanessa Hassouna}


\begin{figure}[!htb]
        \center{\includegraphics[width=1.0\textwidth]
        {img/vanessa_main.png}
        \caption{} Main-Diagramm von Vanessa.lisp}
\end{figure}





%müsste eigendlich wieder ausgelagert werden, kann von mir aus aber auch drin bleiben -v
\subsubsection{find-Object(x z)}
\chapterauthor{Kevin Störmer}
\begin{verbatim}
find-Object()
Beschreibung: Sucht anhand variabler x und z-Achse sowie fest eingestellter
y-Koordinaten nach sichtbaren Objekten. Sollte ein Objekt gefunden werden,
oder die maximale Anzahl an Suchläufen überschritten werden,
terminiert die Funktion. Die Funktion beginnt immer beim zuletzt gesehenen Punkt.
@param: x-axis z-axis
@return: T wenn Objekt gefunden, nil falls nichts zu finden
\end{verbatim}




\newpage
\section{Die Quellcode-Datei: vision\_communicate.lisp (planning\_vision)}

\subsection{Architekturbild}

\chapterauthor{Vanessa Hassouna}



\begin{figure}[!htb]
        \center{\includegraphics[width=0.6\textwidth]
        {img/vision.png}
        \caption{} Architektur der Quellcode-Datei vision\_communicate.lisp}
\end{figure}




\subsection{API}
\chapterauthor{Vanessa Hassouna}
\subsubsection{Serviceclients}
1. '/vision\_suturo/Object\_information' \\
Nimmt Objekt über Kinect wahr und gibt eine 'msg' zurück mit Informationen.\\ \\
2. '/vision\_suturo/objects\_pose' \\
Gibt zu einem Objekt (übergeben als Nummer) und einem Label (übergeben als String) die Pose mit Rotation zurück.

\subsection{Beschreibung des Teilsystems}




\subsubsection{call-Vision-Object-Clouds ()}
\chapterauthor{Vanessa Hassouna}
\begin{verbatim}
call-Vision-Object-Clouds ()

Beschreibung: Der Service von Vision "/vision\_suturo/objects\_information"
wird aufgerufen.

@return: Successfull oder Aborted
\end{verbatim}

%ausgelagert in planning_old
\subsubsection{check-Points-Is-Equal (msg-one msg-two delta))}
\chapterauthor{Kevin Störmer}
\begin{verbatim}
check-Points-Is-Equal (msg-one msg-two delta)

Beschreibung: Vergleicht 2 object_detection-srv:object in
ihren x, y, z Koordinaten anhand eines übergebenen Deltas.
Überschreitet die Differenz der beiden Messages das Delta
wird nil zurückgegeben.

@param: object_detection-srv:object msg-one 
object_detection-srv:object msg-two int delta
@return: T or nil
\end{verbatim}

\section{Die Quellcode-Datei: knowledge-communicate.lisp\\
planning\_knowledge)}

\subsection{Architekturbild}
\chapterauthor{Vanessa Hassouna}

\begin{figure}[!htb]
        \center{\includegraphics[width=1.0\textwidth]
        {img/knowledge.png}
        \caption{} Architektur der Quellcode-Datei knowledge-communicate.lisp}
\end{figure}

\subsection{API}
\chapterauthor{Vanessa Hassouna}
\subsubsection{Serviceclients}

1. 'beliefstate/objects\_to\_pick'\\
Gibt uns zwei Strings zurück. Diese sind leer, wenn kein Objekt mehr gegriffen werden soll oder es kein Objekt mehr zum Greifen gibt.

2. 'beliefstate/gripper\_empty'\\
Wir können erfragen, welcher Gripper frei ist.\\

3. 'kitchen\_mode\_service/get\_fixed\_kitchen\_objects'\\

4. 'storage\_place\_service/storage\_place'\\

5. 'knowledge\_grasp/knowledge\_grasp'\\

6. 'beliefstate/objects\_attached\_to\_gripper\\

\subsubsection{Topics}
1. 'beliefstate/perceive\_action'\\
Wenn ein Objekt wahrgenommen wurde, publishen wir diese Information.


\subsection{Beschreibung des Teilsystems}
\subsubsection{\"Ubersicht}
\chapterauthor{Kevin Störmer}
Die Quellcode-Datei 'knowledge-communicate' im Paket 'planning\_knowledge' ist ausschliesslich für die Kommunikation mit Modulen der Gruppe Knowledge zust\"andig.

\subsubsection{what-object(features)}
\chapterauthor{Kevin Störmer}
\begin{verbatim}
what-object(features)
Beschreibung: Ruft "/svm_classifier/classify_service" Service auf
@param: features
@return: identifikation
\end{verbatim}

\subsubsection{ask-knowledge-where-belongs-object()}
\chapterauthor{Hauke Tietjen}
\begin{verbatim}
ask-knowledge-where-belongs-object()

Beschreibung: Fragt den "storage_place" Service von Knowledge zu welchem
Abstellplatz das gegebene Objekt gehört.

@param object
@return: Abstellplatz als X- und Y-Koordinate in der Mitte, sowie
Breite und Höhe
\end{verbatim}


\subsubsection{how-To-Pick-Objects()}
\chapterauthor{Hauke Tietjen}
\begin{verbatim}
how-To-Pick-Objects()

Beschreibung: Fragt den "knowledge_grasp" Service wie das gegebene Objekt 
gegriffen werden soll. Der Service wird ausnahmsweise auch zum Abstellen 
des Objekts verwendet, weil aufgrund technischer Probleme noch kein
Service zum Abstellen bereitgestellt werden konnte.

@param object
@return: grasp pose
\end{verbatim}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Die Quellcode-Datei: motion-actions.lisp (planning\_motion)}
\subsection{Architekturbild}
\chapterauthor{Vanessa Hassouna}


\begin{figure}[!htb]
        \center{\includegraphics[width=0.6\textwidth]
        {img/motion.png}
        \caption{} Architektur der Quellcode-Datei motion-actions.lisp}
\end{figure}




\subsection{API}
\chapterauthor{Kevin Störmer}
\subsubsection{Actionclients}
1. '/motion' \\
Bewegt die Arme des Pr2 entweder in die Home-Position oder zu einem bestimmten Punkt.
\subsection{Beschreibung des Teilsystems}
\subsubsection{\"Ubersicht}
\chapterauthor{Kevin Störmer}
Die Quellcode-Datei 'motion-actions' im Paket 'planning\_motion'  ist ausschliesslich für die Kommunikation mit Modulen der Gruppe Motion zuständig. Dabei wird die Action '/motion' einmal für die Home-Position des Pr2, zum Bewegen des Armes zu einem bestimmten Punkt und zum Greifen genutzt.


\subsubsection{call-Motion-Move-Arm-Homeposition()}
\chapterauthor{Vanessa Hassouna und Hauke Tietjen}
\begin{verbatim}
call-Motion-Move-Arm-Homeposition()

Beschreibung: Ruft den Actionserver von Motion auf und
sendet das 'Command 2'.Dieses Command lässt die Arme des Pr2 
in die Homeposition fahren.

@return: Successfull oder Aborted
\end{verbatim}



\subsubsection{call-Motion-Move-Arm-To-Point \\
(point-center-of-object \&optional (x 3))
}
\chapterauthor{Vanessa Hassouna und Hauke Tietjen}
\begin{verbatim}
call-Motion-Move-Arm-To-Point (point-center-of-object \&optional (x 3))

Beschreibung: Ruft den Actionserver von Motion auf und 
sendet den Befehl einen Arm zu dem point-center-of-object zu fahren.
@param point-center-of-object &optional (x 3)
@return: Successfull oder Aborted
\end{verbatim}

\newpage
\section{Die Quellcode-Datei: movement.lisp (planning\_move)}
\subsection{Architekturbild}
\chapterauthor{Kevin Störmer}


\begin{figure}[!htb]
        \center{\includegraphics[width=0.8\textwidth]
        {img/diag_planning_move.png}
        \caption{} Architektur der Quellcode-Datei movement.lisp}
\end{figure}


\subsection{API}
\chapterauthor{Kevin Störmer}
\subsubsection{Actionclients}
1. 'head\_traj\_controller/point\_head\_action' \\
Bewegt den Kopf des Pr2 in Richtung eines Punktes.\\ \\
2. 'nav\_pcontroller/move\_base' \\
Bewegt die Basis des Pr2 in Richtung eines Punktes.
\subsection{Beschreibung des Teilsystems}

\subsubsection{\"Ubersicht}
\chapterauthor{Kevin Störmer}
Die Quellcode-Datei actions.lisp im Paket 'planning\_motion'  ist ausschliesslich für die Kommunikation mit Gruppe Motion zuständig. Dabei wird die Action '/motion' einmal für die Home-Position des Pr2 und zum Bewegen des Armes zu einem bestimmten Punkt genutzt.



\subsubsection{move-head (x y z)}
\chapterauthor{Kevin Störmer}

\begin{verbatim}
move-Head (x y z)
Beschreibung: Lässt den Rr2-Kopf bewegen, dabei wird der Service 
"head_traj_controller/point_head_action" angesprochen.
x,y, und z werden als Koordinaten ausgehend von base_link behandelt.
@param: x y z
@return: Succesfull oder Aborted
\end{verbatim}



\subsubsection{move-Base-To-\\
Point (x y z angle)}
\chapterauthor{Vanessa Hassouna}
\begin{verbatim}
move-Base-To-Point (x y z angle)

Beschreibung: Anhand des Actionservice "nav\_pcontroller/move\_base" 
wird die Basis bewegt. Der Punkt und die Rotation sind in Frame "map" anzugeben.

@param: x y z angle
@return: Successfull oder Aborted
\end{verbatim}


\subsubsection{move-Robo-Into-Homeposition ()}
\chapterauthor{Vanessa Hassouna}
\begin{verbatim}
move-Robo-Into-Homeposition ()

Beschreibung: move-Base-To-Point wird aufgerufen mit 
von uns vordefinierten Punkten, die als Homeposition dienen.

@return: Succesfull oder Aborted
\end{verbatim}



\subsubsection{init-Action-Client-Base ()}
\chapterauthor{Vanessa Hassouna}
\begin{verbatim}
init-Action-Client-Base ()

Beschreibung: Diese Hilfsfunktion dient dazu, 
den Action-Client zu initialisieren.

@return: Successfull oder Aborted
\end{verbatim}

\subsubsection{get-Action-Client-Base ()}
\chapterauthor{Vanessa Hassouna}
\begin{verbatim}
get-Action-Client-Base ()

Beschreibung: Diese Hilfsfunktion dient dazu, den Action-Client zu erhalten.

@return: Successfull oder Aborted
\end{verbatim}





\section{Die Quellcode-Datei: external-logic.lisp (planning\_logic)}
\subsection{Architekturbild}
\chapterauthor{Vanessa Hassouna}


\begin{figure}[!htb]
        \center{\includegraphics[width=0.8\textwidth]
        {img/externallogic.png}
        \caption{} Architektur der Quellcode-Datei movement.lisp}
\end{figure}


\subsection{API}
\chapterauthor{Vanessa Hassouna}
\subsubsection{Topics}
1. 'joint\_states' \\
Subscribed das Topic und speichert die Werte des Grippers.\\
 
2. 'robot\_pose' \\
Subscribed das Topic und gibt uns die Position des Roboters zurück.
\subsection{Beschreibung des Teilsystems}

\subsubsection{\"Ubersicht}
\chapterauthor{Vanessa Hassouna}
Die Quellcode-Datei 'external-logic.lisp' im Paket 'planning\_logic' ist für reine logische Funktionen vorgesehen.



\subsubsection{transformation-Vision-Point (pose amount\\
\&optional (endFrame '/base\_footprint')}
\chapterauthor{Vanessa Hassouna}
\begin{verbatim}
transformation-Vision-Point (pose amount &optional (endFrame "/base\_footprint")

Beschreibung: Das gegebene Objekt wird von dem ausgehenden Frame
in ein optionales Frame umgewandelt.

@param: pose, amount und optional endFrame
@return: Liefert eine transformierte tf-point-stamped
\end{verbatim}


\subsubsection{publish-pose (label object\_pose)}
\chapterauthor{Kevin Störmer}
\begin{verbatim}
publish-pose (label object_pose)

Beschreibung: Published eine knowledge_msgs/PerceivedObject-msg

@param: string label, PoseStamped object_pose
@return: int amountOfListeners
\end{verbatim}


\subsubsection{ publish-Text (string)}
\chapterauthor{Vanessa Hassouna}
\begin{verbatim}
publish-Text (string)

Beschreibung: Publish a text in RVIZ with a given string.

@param:string
\end{verbatim}


\subsubsection{publish-Pose-JaMilch ()}
\chapterauthor{Vanessa Hassouna}
\begin{verbatim}
publish-Pose-JaMilch ()

Beschreibung: This function is only for the demo scenario purpose.
Here a "JaMilch" is percieved and published to Team Knowledge also a new param
will be created on the Ros Param Server.

@param:string
\end{verbatim}



\subsubsection{catch-Transformation \\
(transform-listener tf-point-stamped endFrame)}
\chapterauthor{Vanessa Hassouna}
\begin{verbatim}
catch-Transformation (transform-listener tf-point-stamped endFrame)

Beschreibung: Hier findet die eigentliche Transformation, von der
Funktion transformation-Vision-Point 
pose amount &optional (endFrame "/base_footprint"), statt.

@param: transform-listener tf-point-stamped endFrame
@return: Liefert eine transformierte tf-point-stamped
\end{verbatim}

\subsubsection{disassemble-graspindividual-response (msg)}
\chapterauthor{Kevin Störmer}
\begin{verbatim}
disassemble-graspindividual-response (msg)

Beschreibung: Löst die Y-Koordinate einer GraspPose heraus.

@param: GraspPoseMessage msg
@return: float y
\end{verbatim}


\subsubsection{transformation-XYZ (x y z startFrame\\
\&optional (endFrame '/base\_footprint'))}
\chapterauthor{Vanessa Hassouna}
\begin{verbatim}
transformation-XYZ (x y z startFrame &optional (endFrame "/base_footprint"))

Beschreibung: Transforms a point with standart ( 0 0 0 1) quaternion.

@param: x y z startFrame &optional (endFrame "/base_footprint")
@return: Liefert eine transformierte tf-pose-stamped
\end{verbatim}


\subsubsection{ transformation-Pose-Stamped \\
(pose \&optional (endFrame '/base\_footprint')) }
\chapterauthor{Vanessa Hassouna}
\begin{verbatim}
 transformation-Pose-Stamped (pose &optional (endFrame "/base_footprint")) 

Beschreibung: Transform a Pose-Stamped-msgs with an optional Frame,\\
 default is base_footprint.

@param: x y z startFrame &optional (endFrame '/base_footprint')
@return: Liefert eine transformierte tf-pose-stamped
\end{verbatim}




\subsubsection{make-Object-Pose-For-Handshake (label)}
\chapterauthor{Vanessa Hassouna}
\begin{verbatim}
make-Object-Pose-For-Handshake (label)

Beschreibung: Making a msg from the point of an object for \\
the handshake function.

@param: label
@return: T oder Nil
\end{verbatim}

\subsubsection{ percieve-Objetcs ()}
\chapterauthor{Vanessa Hassouna}
\begin{verbatim}
percieve-Objetcs ()

Beschreibung: Calling vision_sutuo service to percieve objects,\\
while looping for the array-total-size through all the percieved \\
objects will be published and saved on to the Paramserver.
Another param named counter is logging how many objects has been seen.

@return: T oder Nil
\end{verbatim}

\subsubsection{ percieve-Objects-And-Search (label)}
\chapterauthor{Vanessa Hassouna}
\begin{verbatim}
 percieve-Objects-And-Search (label)

Beschreibung: searching for the given object if its still there.

@param: label
@return: T oder Nil
\end{verbatim}


\subsubsection{get-Information-About-Object (label)}
\chapterauthor{Vanessa Hassouna}
\begin{verbatim}
get-Information-About-Object (label)

Beschreibung: Transforming the point of the label to frame "map" return \\
value is the transformed y value.

@param: label
@return: Transformed pose
\end{verbatim}

\subsubsection{save-Object (label object\_pose)}
\chapterauthor{Vanessa Hassouna}
\begin{verbatim}
save-Object (label object_pose)

Beschreibung: The given object (label) will be set as a new param on the Paramserver
, but keep in mind only the Point will be saved. The label will be the name of the param.

@param: label object_pose
@return: T oder Nil
\end{verbatim}


\subsubsection{init-Marker ()}
\chapterauthor{Vanessa Hassouna}
\begin{verbatim}
init-Marker ()

Beschreibung: Subscribed das Topic '/visualization_markers'.

@return: geometry_msgs/Pose
\end{verbatim}

\subsubsection{init-pr2()}
\chapterauthor{Vanessa Hassouna}
\begin{verbatim}
init-pr2 ()

Beschreibung: Subscribed das Topic '/amcl_status'.

@return: geometry_msgs/Pose
\end{verbatim}

\subsubsection{pose-cb (msg)}
\chapterauthor{Vanessa Hassouna}
\begin{verbatim}
pose-cb (msg)

Beschreibung: Callback-Funktion für die Funktion init-pr2().

@param: msg
@return: geometry_msgs/Pose
\end{verbatim}

\subsubsection{move-pr2 (x y z)}
\chapterauthor{Vanessa Hassouna}
\begin{verbatim}
move-pr2 (x y z)

Beschreibung: Bewegt den Roboter zu einem Punkt, wenn dieser hinter\\
dem Roboter liegt begiebt dieser sich erst in eine Safeposition.
@param: x y z 
@return: Successfull oder Aborted
\end{verbatim}

\subsubsection{angle-From-Pr2-Pose-To-Point (x-goal y-goal z-goal)}
\chapterauthor{Vanessa Hassouna}
\begin{verbatim}
angle-From-Pr2-Pose-To-Point (x-goal y-goal z-goal)

Beschreibung: Die Funktion arbeitet mit Hilfe des Packets Cram-Language um 
einen Winkel zu berechnen, wie der Pr2 sich aus der aktuellen Position 
in der Welt drehen muss, damit er einen gegebenen Punkt erreichen kann.
Da der Roboter sich in dem Frame "map" bewegt, muss der Winkel noch mit der 
aktuellen Orientierung des Pr2 subtrahiert werden, so dass am Ende
ein Winkel resultiert der in dem Frame "map" funktioniert.

@param: x-goal y-goal z-goal 
@return: angle 
\end{verbatim}


\subsubsection{should-Robo-Use-Left-Or-Right-Arm(pose amount \\
\&optional (endFrame '/base\_footprint'))}
\chapterauthor{Vanessa Hassouna}
\begin{verbatim}
should-Robo-Use-Left-Or-Right-Arm (pose amount &optional
(endFrame "/base_footprint"))

Beschreibung: Anhand der Y-Achse entscheidet der Roboter
ob er den linken oder rechten Arm bewegen soll.

@param: pose object-number &optional endFrame
@return: 3 oder 2 (3 = links 2 = Rechts)
\end{verbatim}

\subsubsection{ try-To-Grab-Or-Place-Different-Location\\
(x y z w label command)}
\chapterauthor{Vanessa Hassouna}
\begin{verbatim}
try-To-Poke-Different-Location\\
(point-for-motion number-for-arm)

Beschreibung: Die Funktion lässt den Roboter 
anhand vorgegebener Positionen seine Drehung sowie seine
Position ändern, wenn er ein Objekt nicht erreichen kann.
Achtung die X-, Y-, Z- und W-Werte müssen in dem Frame "map" angegeben werden.
Der Roboter selber bewegt sich in dem Map base\_footprint.

@param: x, y, z, w, label, command
@return: T oder Nil


\end{verbatim}

\subsubsection{  grab-Or-Place-Object (label x y angle arm-first \\
arm-first-homeposi grab-string \&optional \\
arm-second arm-second-homeposi)}
\chapterauthor{Vanessa Hassouna}
\begin{verbatim} 
grab-Or-Place-Object (label x y angle arm-first 
arm-first-homeposi grab-string &optional arm-second arm-second-homeposi)

Beschreibung: Trying to grab or place, if the given grab-string
 is equal to grab the pr2 will also try to grab it with another arm and different places.

@param: label x y angle arm-first arm-first-homeposi grab-string &optiona\\
arm-second arm-second-homeposi
@return: T oder Nil
\end{verbatim}



\subsubsection{ trying-To-Grab (label x y angle arm-first arm-first-homeposi \\
grab-string arm-second arm-second-homeposi)}
\chapterauthor{Vanessa Hassouna}
\begin{verbatim} 
 trying-To-Grab (label x y angle arm-first arm-first-homeposi
 grab-string arm-second arm-second-homeposi)

Beschreibung: Calling grab-or-place-object, \\
if the counter is bigger than 0 the pr2 wil first change the Position.

@param: Label x y angle arm-first arm-first-homeposi \\
grab-string arm-second arm-second-homeposi
@return: T oder Nil
\end{verbatim}

\subsubsection{how-Many-Gripper ()}
\chapterauthor{Vanessa Hassouna}
\begin{verbatim} 
how-Many-Gripper ()

Beschreibung: Looping through the emtpy-gripper server and\\
setting the variable left and right to T(1) or nil (0).

@return: T oder Nil
\end{verbatim}


\subsubsection{calculate-Object-And-Pr2-Distance (label)}
\chapterauthor{Vanessa Hassouna}
\begin{verbatim} 
calculate-Object-And-Pr2-Distance (label)

Beschreibung: Calculating the distance from the given\\
object to 2 different locations.

@param: label
@return: T oder Nil
\end{verbatim}


\subsubsection{distance (xa ya xb yb)}
\chapterauthor{Vanessa Hassouna}
\begin{verbatim}
distance (xa ya xb yb)

 Beschreibung:calctulating the distance between two 2d-vectors.

@param: xa ya xb yb (vektor1 vektor2)
@return: distance
\end{verbatim}


\subsubsection{set-First-Posi ()}
\chapterauthor{Vanessa Hassouna}
\begin{verbatim}
set-First-Posi ()

 Beschreibung: Setting global x y and angle to the first position.

@return: T oder Nil
\end{verbatim}


\subsubsection{set-Second-Posi ()}
\chapterauthor{Vanessa Hassouna}
\begin{verbatim}
set-Second-Posi ()

 Beschreibung: Setting global x y and angle to the second position.

@return: T oder Nil
\end{verbatim}





\subsubsection{init-gripper-states ()}
\chapterauthor{Kevin Störmer}
\begin{verbatim}
init-gripper-states ()
Beschreibung: Subscribed das Topic '/joint_states' und gibt als Callback-Funktion
'is-gripper-filled (msg)' an.
\end{verbatim}

\subsubsection{is-gripper-filled (msg)}
\chapterauthor{Kevin Störmer}
\begin{verbatim}
is-gripper-filled (msg)
Beschreibung: Aktualisiert alle 0.01 Sekunden die Fluents *gripper-righ-state-fluent* 
und *gripper-left-state-fluent* um T oder nil. Dabei wird anhand der Joint-States der
Gripper-Joints 'r_gripper_joint' und 'l_gripper_joint' entschieden
ob jeder Gripper des Pr2 gefüllt ist, oder nicht. Diese werte werden
aus den Joint-States geparsed.
@param: sensor_msgs/JointState msg 
\end{verbatim}



\section{Die Quellcode-Datei: objects.lisp (planning\_objects)}
\subsection{Architekturbild}
\chapterauthor{Hauke Tietjen}



\begin{figure}[!htb]
	\center{\includegraphics[width=1.0\textwidth]
		{img/diag_planning_objects.png}
		\caption{} Architektur der Quellcode-Datei planning-objects.lisp}
\end{figure}


\subsection{API}
\chapterauthor{Hauke Tietjen}
\subsubsection{Serviceclients}
1. '/storage\_place\_service/storage\_place' \\
Gibt den Ablageplatz des Objekts zur\"uck.\\ \\
2. '/object\_grasping/grasp\_pose' \\
Gibt die Greifpose des für die Koordinaten zurück.\\
3. '~location\_marker' \\
Publiziert Marker zur Visualisierung von Landezonen.
\subsection{Beschreibung des Teilsystems}
\subsubsection{\"Ubersicht}
\chapterauthor{Hauke Tietjen}
Die Datei 'objects' aus dem Paket 'planning\_objects' dient dazu, aus den von Knowledge vorgegebenen Ablageorten für Objekte, Punkte zu bestimmen, die geeignet sind um Objekte auf ihnen abzustellen. Dabei werden Platzmangel, Kollision und das Herunterfallen von Objekten berücksichtigt. Zusätzlich lassen sich die finalen Positionen in Rviz visualisieren.

\subsubsection{calculate-landing-zone()}
\chapterauthor{Hauke Tietjen}
\begin{verbatim}
calculate-landing-zone()

Beschreibung: Die Funktion fragt Knowledge an welchen Ablageort das Objekt gehört und
verkleinert ihn. Dadurch soll das Platzieren von Objekten auf Tischkanten
verhindert werden. Danach wird die Funktion fill-landing-zone-horizontally
aufgerufen, die eine Pose zurück gibt. Außerdem wird Knowledge nach der 
Z-Achse und Orientierung gefragt, wo das Objekt gegriffen werden soll. 
Das wird in den Frame '/map' transformiert und die X- und Y-Achse
werden mit den neuen Werten aus fill-landing-zone-horizontally überschrieben.

@param object
@return Gibt die Pose eines Grippers in /map zurück um ein Objekt abzustellen
\end{verbatim}

\subsubsection{fill-landing-zone-horizontally()}
\chapterauthor{Hauke Tietjen}
\begin{verbatim}
fill-landing-zone-horizontally()

Beschreibung: Es wird festgestellt in welchen Abstellbereich das Objekt gehört.
Dann wird der Bereich mit dem gegebenen Objekt befüllt.
Dies geschieht von positiv nach negativ auf der Y-Achse, im Frame '/map'.

@param position width height
@return Gibt eine Pose in /map zurück um ein Objekt abzustellen
\end{verbatim}

\subsubsection{calculate-landing-zone-visualized()}
\chapterauthor{Hauke Tietjen}
\begin{verbatim}
calculate-landing-zone-visualized()

Beschreibung: Die Funktion führt calculate-landing-zone() aus und
visualisiert das Ergebnis, mit visualize-landing-zone().

@param object
@return Gibt eine Pose in /map zurück um ein Objekt zu greifen
\end{verbatim}

\subsubsection{visualize-landing-zone()}
\chapterauthor{Hauke Tietjen}
\begin{verbatim}
visualize-landing-zone()

Beschreibung: Die Funktion führt vis-init() und publish-pose() aus
um eine Pose zu visualisieren.

@param pose
@return 
\end{verbatim}

\subsubsection{vis-init()}
\chapterauthor{Hauke Tietjen}
\begin{verbatim}
vis-init()

Beschreibung: Die Funktion startet den Marker-publisher.

@param 
@return 
\end{verbatim}

\subsubsection{publish-pose()}
\chapterauthor{Hauke Tietjen}
\begin{verbatim}
publish-pose()

Beschreibung: Es wird ein Marker an der angegebenen Pose publiziert,
mit ID und den Dimensionen aus height und width.

@param pose id height width
@return 
\end{verbatim}

\section{Die Quellcode-Datei: error-hierarchy.lisp (planning\_error)}
\subsection{Architekturbild}
\chapterauthor{Hauke Tietjen}
\begin{figure}[!htb]
	\center{\includegraphics[width=0.4\textwidth]
		{img/diag_planning_error.png}
		\caption{} Architektur der Quellcode-Datei error-hierarchy.lisp}
\end{figure}


\subsection{API}
\chapterauthor{Hauke Tietjen}
\subsubsection{Serviceclients}
keine
\subsection{Beschreibung des Teilsystems}
\subsubsection{\"Ubersicht}
\chapterauthor{Hauke Tietjen}
Die Datei 'error-hierarchy' aus dem Paket 'planning\_error' legt die Hierarchie der Fehler fest.

\subsubsection{custom-error()}
\chapterauthor{Hauke Tietjen}
\begin{verbatim}
custom-error()

Beschreibung: Dieser Standardfehler erbt von 'error' während
alle weiteren Fehler von diesem erben. 

@param message
@return 
\end{verbatim}

\subsubsection{vision-error()}
\chapterauthor{Hauke Tietjen}
\begin{verbatim}
vision-error()

Beschreibung: Diese Fehlerklasse beschreibt Fehler, die im
Zusammenhang mit Vision stehen. Vor der 
Fehlernachricht wird 'vison error:' ausgegeben. 

@param
@return 
\end{verbatim}

Die Conditions 'motion-error', 'move-error', 'knowledge-error' und 'objects-error' funktionieren analog zu 'vison-error' und definieren weitere Fehler, für andere Bereiche.
\newpage
\section{Die Quellcode-Datei: interaction.lisp (planning\_interaction)}
\subsection{Architekturbild}
\chapterauthor{Kevin Störmer}



\begin{figure}[!htb]
	\center{\includegraphics[width=1.0\textwidth]
		{img/diag_planning_interaction.png}
		\caption{} Architektur der Quellcode-Datei planning-interaction.lisp}
\end{figure}



\subsection{API}
\chapterauthor{Kevin Störmmer}
\subsubsection{Topics}
1. '/planning\_interaction/wrench\_force\_magnitude' \\
Published auf dem Topic den Betrag des Kraeftevektors des Handgelenkes.\\ \\
2. '/planning\_interaction/wrench\_handshake\_detection' \\
Published auf dem Topic eine 15 wenn Ein Handshake wahrgenommen wird.\\ \\
3. '/beliefstate/grasp\_object\_human\_interaction' \\
Published für Knowledge eine Message, wenn ein Objekt von einem Menschen in den Gripper gelegt wird.\\ \\
4. '/beliefstate/delete\_object\_human\_interaction' \\
Published für Knowledge eine Message wenn ein Objekt aus dem Gripper des Pr2 entfernt wird.\\ \\
5. '/ft/l\_gripper\_motor\_zeroed' \\
Subscribed auf den Topic um die kalibrierten Handgelenkswerte des Pr2 zu bekommen.\\ \\
\subsubsection{Actionserver}
1. '/sound\_play' \\
Nutzt die Soundplay Action, des Sound\_Play Paketes um dem Pr2 eine Stimme zu synthetisieren.\\ \\
\subsection{Beschreibung des Teilsystems}
\subsubsection{\"Ubersicht}
\chapterauthor{Kevin Störmer}
Die Datei interaction.lisp ist zuständig dafür, Situationen aufzuloesen, die nicht vom Code abgedeckt werden, oder die für den Roboter physikalisch nicht möglich sind.\\
Dabei soll vorallem auf Sicherheit der Interaktion und Einfachheit in der Implementation geachtet werden. Fokus liegt dabei auch auf dem Feststellen von Handshake-Actions sodass dem Menschen eine Interaktionsschnittstelle gegeben wird.\\
Wie in der folgenden Illustration gezeigt, werden Sicherheitskritische Aspekte dabei von den oeffentlichen Funktionen getrennt.
\begin{figure}[!htb]
	\center{\includegraphics[width=1.0\textwidth]
		{img/ablauf_planning_interaction.png}
		\caption{} Ablauf der Initialisierung}
\end{figure}

\subsubsection{init-interaction()}
\chapterauthor{Kevin Störmer}
\begin{verbatim}
init-interaction()
Beschreibung: Initialisiert das Paket. Startet alle Subkomponenten, 
und teilt dem Nutzer mit falls Abhängigkeiten fehlen.
@return true wenn richtig initalisiert
\end{verbatim}

\subsubsection{ask-human-to-move-object()}
\chapterauthor{Kevin Störmer}
\begin{verbatim}
ask-human-to-move-object()
Beschreibung: Fehlerfallabdeckung für den Fall das der Pr2 ein Objekt
nicht greifen kann. Pr2 zeigt dann auf das Objekt, und ruft 
einen Menschen zu Hilfe.
Dieser kann dann das Objekt in den Gripper des Pr2 legen 
und ihm die Hand schütteln. Der Roboter nimmt dann seine Arbeit 
wieder auf.
@param pose label force moving command statement1 statement2
@return Kein definierter Rückgabewert
\end{verbatim}

\subsubsection{ask-human-to-take-object ()}
\chapterauthor{Kevin Störmer}
\begin{verbatim}
ask-human-to-take-object ()
Beschreibung: Fehlerfallabdeckung für den Fall das der Pr2 ein Objekt 
nicht ablegen kann. Der Pr2 fährt dann an einen Ort an dem er gut 
mit dem Menschen interagieren kann. Dort streckt er seine Arme aus 
und verweist auf den Gripper in dem das Objekt steckt, 
welches er loswerden möchte. \\
Nach einem Händeschütteln, wird das Objekt freigegeben 
und als abgelegt behandelt. Der PR2 nimmt dann seine Arbeit wieder auf.
@param label moving-command
@return Kein definierter Rückgabewert
\end{verbatim}

\subsubsection{check-gripper()}
\chapterauthor{Kevin Störmer}
\begin{verbatim}
check-gripper()
Beschreibung: Anpassbare Funktion für beliebige Fehlerfälle. Es kann 
eine Funktion übergeben werden, welche dann in einer sicheren 
Umgebung aufgerufen wird. Nebenbei werden die beiden Gripper gecheckt, 
sodass nichts aus dem Gripper fallen kann währenddessen. Wenn kein 
Fehlschlag passiert, läuft die mitgegebene Funktion normal durch.
@param errormsg func args r l)
@return Kein definierter Rückgabewert
\end{verbatim}

\subsubsection{decide-gripper()}
\chapterauthor{Kevin Störmer}
\begin{verbatim}
decide-gripper()
Beschreibung: Da Motion merkwürdige Magic-Numbers für ihre Actions verwendet, 
wird diese Funktion gebraucht um von der Einen auf die Andere schliessen zu 
können.
@param moving-command
@return magic-number für die Gripper-Action
\end{verbatim}

\subsubsection{drive-to-human ()}
\chapterauthor{Kevin Störmer}
\begin{verbatim}
drive-to-human ()
Beschreibung: Fährt zu Ort für menschliche Interaction an der Iai Holz-Küche.
@param -
@return Kein definierter Rückgabewert
\end{verbatim}

\subsubsection{say ()}
\chapterauthor{Kevin Störmer}
\begin{verbatim}
say ()
Beschreibung: Nutzt das sound\_play Paket um einen String in 
gesprochene Sprache zu synthetisieren und auf dem Pr2 auszugeben.
@param message
@return Kein definierter Rückgabewert
\end{verbatim}

\subsubsection{wait-for-handshake ()}
\chapterauthor{Kevin Störmer}
\begin{verbatim}
wait-for-handshake ()
Beschreibung: Anpassbare Funktion zur Konfiguration eigener Fehlerfälle. 
Pr2 führt stoppt seine Aufgabe, und gibt mitgegebene Errormessage aus. 
Danach wartet er auf einen Handshake und führt dann die mitgegebene 
Funktion aus.
@param func args errormsg
@return Kein definierter Rückgabewert
\end{verbatim}

\subsubsection{build-pointing-pose ()}
\chapterauthor{Kevin Störmer}
\begin{verbatim}
build-pointing-pose ()
Beschreibung: Baut die Pointing-Pose aus den einzelnen Berechnungen 
zusammen. Diese Pose wird gebraucht um auf ein Objekt zu zeigen.
@param pose
@return Pose die auf das Objekt zeigt
\end{verbatim}

\subsubsection{get-pointing-pose ()}
\chapterauthor{Kevin Störmer}
\begin{verbatim}
get-pointing-pose ()
Beschreibung: Baut die Pointing-Pose zusammen, ohne Quaternion. 
Verarbeitet die Daten aus 'get-vektor-pr2-reachable' 
(beschrieben nachfolgend). 
@param pose
@return Pointing-Pose ohne Quaternion
\end{verbatim}

\subsubsection{get-vektor-pr2-reachable ()}
\chapterauthor{Kevin Störmer}
\begin{verbatim}
get-vektor-pr2-reachable ()
Beschreibung: Rechnet in Abhängigkeit von Arm und Gegebenem 
Vektor den nähesten Punkt aus den der Pr2 erreichen kann
@param vektor
@return erreichbarer Punkt
\end{verbatim}

\subsubsection{calculate-pointing-quaternion ()}
\chapterauthor{Kevin Störmer}
\begin{verbatim}
calculate-pointing-quaternion ()
Beschreibung: Berechnet aus einer gegebenen Object-Pose das 
Quaternion was benötigt wird um auf ein Objekt zu zeigen. 
@param object-pos
@return quaternion, welches zum pointen gebraucht wird
\end{verbatim}

\subsubsection{get-directional-vector ()}
\chapterauthor{Kevin Störmer}
\begin{verbatim}
get-directional-vector ()
Beschreibung: Berechnet aus 2 Punkten im Raum den Richtungsvektor in Richtung 
des 2ten Punktes.
@param Vektor1 Vektor2
@return Richtungsvektor
\end{verbatim}

\subsubsection{vectors-to-matrix ()}
\chapterauthor{Kevin Störmer}
\begin{verbatim}
vectors-to-matrix ()
Beschreibung: Baut und berechnet aus 3 Punkten oder Vektoren im Raum eine 
Transformationsmatrix.
@param Vektor-x Vector-y Vector-z
@return 3x3 Tansformationsmatrix
\end{verbatim}

\newpage

\subsubsection{calculate-wrench-magnitude ()}
\chapterauthor{Kevin Störmer}
\begin{verbatim}
calculate-wrench-magnitude ()
Beschreibung: Berechnet Betrag der Wrench-Force-Daten und published sie. 
Ausserdem wird immer genau dann auf dem Handshake-detection gepublished, 
wenn ein Handschlag festgestellt wird. (Callback-Funktion)
@param msg
@return Kein definierter Rückgabewert
\end{verbatim}

\begin{figure}[!htb]
	\center{\includegraphics[width=1.0\textwidth]
		{img/detection_planning_interaction.png}
		\caption{} Detection eines Handshakes, wie in calculate-wrench-magnitude}
\end{figure}


\subsubsection{put-object-in-hand ()}
\chapterauthor{Kevin Störmer}
\begin{verbatim}
put-object-in-hand ()
Beschreibung: Simple Methode um Knowledge mitzuteilen, wenn ein Objekt 
in den Gripper gelegt wird.
@param gripper label
@return Kein definierter Rückgabewert
\end{verbatim}

\subsubsection{drop-object-in-hand ()}
\chapterauthor{Kevin Störmer}
\begin{verbatim}
drop-object-in-hand ()
Beschreibung: Simple Methode um Knowledge mitzuteilen, wenn ein Objekt 
aus dem Gripper gelegt wird.
@param label
@return Kein definierter Rückgabewert
\end{verbatim}

\newpage

\section{Die Quellcode-Datei: old.lisp (planning\_old)}

\subsubsection{disassemble-Vision-Call (visionclouds)}
\chapterauthor{Vanessa Hassouna}
\begin{verbatim}
disassemble-Vision-Call (visionclouds)

Beschreibung: Extrahiert alle Informationen aus der 
Visioncloud und speichert normal_features, color_features, 
object_amount und object_pose auf dem Parameterserver.
 
@param: visionclouds
@return: object_pose
\end{verbatim}



\subsubsection{set-Params-Features \\
(normal-s color-s normal-e color-e amount)}
\chapterauthor{Vanessa Hassouna}
\begin{verbatim}
set-Params-Features (normal-s color-s normal-e color-e amount)

Beschreibung: Eine Hilfsfunktion für die Funktion disassemble-Vision-Call, 
um aus normal_features und color_features konkateniert ein features-X zu 
erstellen. (Knowledge benötigt ein Array in dem zuerst color_features 
vorkommen und direkt im Anschluss normal_features)

@param: normal-s color-s normal-e color-e amount
@return: Nil
\end{verbatim}













\newpage
\section*{Methodendokumentation Gruppe Knowledge}
\section{knowledge\_grasp}
\chapterauthor{Max-Phillip Bahr}
Das Knowledge\_Grasp-Paket dient dazu, die GraspPose zu einem erkannten Gegenstand aus der Ontologie auszulesen. Die GraspPose dient dazu dem PR2 mitzuteilen wie ein Gegenstand zu greifen ist.

\begin{figure}[!htb]
        \center{\includegraphics[width=\textwidth]
        {figures/knowledge_grasp_architektur.png}
        \caption{\label{fig:knowledge_grasp_service_node} Architektur der knowledge\_grasp\_service-node}}
\end{figure}
      
\subsection{Beschreibung des Teilsystems}
\chapterauthor{Max-Phillip Bahr}

\subsubsection{find\_grasp\_pose - Prolog}
\begin{spverbatim}
find_grasp_pose(ObjectClassLabel, Position, Quaternion)
@param ObjectClassLabel Das Objektlabel des zu greifenden Objekts 
       (z.B. suturo_object:'JaMilch')
@param Position Position der GraspPose als Prolog Liste (z.B. [0.0, 0.0, 0.0])
@param Quaternion Orientation der GraspPose also Prolog Liste 
       (z.B. [0.0 0.0 0.0 1.0])

Description : Liest zu einem gegebenen Objektlabel eine zugehörige GraspPose 
aus der Ontologie aus. Es wird bei der eingespeicherten GraspPose von der 
Translation des Grippers zum Mittelpunkt des Objekts ausgegangen.
\end{spverbatim}

\subsubsection{createQuery - C++}
\begin{spverbatim}
std::string createQuery(std::string object_label)
@param object_label Das Objektlabel.
@return Den aus dem Objektlabel zusammengesetzten Query-String.

Description : Baut aus dem Objektlabel einen Query-String für die 
Prolog-Methode find_grasp_pose zusammen.
\end{spverbatim}

\subsubsection{find\_grasp\_pose - C++}
\begin{spverbatim}
bool find_grasp_pose(knowledge_msgs::GraspIndividual::Request  &req, 
                             knowledge_msgs::GraspIndividual::Response &res)
@param req Adresse des Request Objekts von GraspIndividual das bearbeitet 
           werden soll.
@param res Adresse zum schreiben des Response Objekts von GraspIndividual.
@return true wenn Methode erfolgreich, false wenn nicht.

Description : Die Methode liest das Objektlabel aus req aus und baut mit 
createQuery einen QueryString, der mit der once-Methode an den Prolog Server 
geschickt wird. Wenn Prolog mit find_grasp_pose (Prolog Methode) eine Lösung 
zur Query findet (also die GraspPose aus der Ontologie ausliest), wird diese in 
das Response Objekt geschrieben und true zurückgegeben. Wenn nicht wird 
eine Fehlermeldung ausgegeben und false zurückgegeben.
\end{spverbatim}

\section{svm\_classifier}
\chapterauthor{Alexander Haar}
Das svm\_classifier-Paket dient zur Ermittlung eines Objektlabels anhand eines Featurevektors.

\subsection{svm\_classifier}
\begin{figure}[!htb]
        \center{\includegraphics[width= 10cm]
        {figures/svm_classifier.png}
        \caption{\label{fig:classifier} Architektur der svm\_classifier-node}}
\end{figure}

\subsection{svm\_training\_data\_collector}
\begin{figure}[!htb]
        \center{\includegraphics[width= 5cm]
        {figures/svm_training_data_collector.png}
        \caption{\label{fig:training_data} Architektur der svm\_training\_data\_collector-node}}
\end{figure}

\subsection{svm\_trainer}
\begin{figure}[!htb]
        \center{\includegraphics[width=5cm]
        {figures/svm_trainer.png}
        \caption{\label{fig:trainer} Architektur der svm\_trainer-node}}
\end{figure}
      
\subsection{Beschreibung des Teilsystems}
\chapterauthor{Alexander Haar}

\subsubsection{main - Python}
\begin{verbatim}
if __name__ == '__main__'
Beschreibung: Lädt die svm_model.sav Datei und startet den Classify- Service.
\end{verbatim}

\subsubsection{save\_training\_set - Python}
\begin{verbatim}
def save_training_set()
Beschreibung: Nachdem alle Trainingsdaten gesammelt wurden, speichert diese 
Funktion alle Daten als sav- Datei.
\end{verbatim}

\subsubsection{get\_corresponding\_normals\_histogram\_file - Python}
\begin{verbatim}
def get_corresponding_normals_histogram_file(color_histogram_file)
Beschreibung: Findet zu einem Farbhistogramm das entsprechende Normalshistogramm. 
@param color_histogram_file: Das Farbhistogramm.
@return: Das korrespondierende Normalshistogramm.
\end{verbatim}

\subsubsection{get\_features - Python}
\begin{verbatim}
def get_features(label, color_histogram_file)
Beschreibung: Ermittelt anhand des Labels der Klasse und der Farbhistogrammdatei 
den Featurevektor
@param label: Das Label der Klasse.
@param color_histogram_file: Die Farbhistogrammdatei.
@return: Der resultierende Featurevektor.
\end{verbatim}

\subsubsection{collect\_training\_data - Python}
\begin{verbatim}
def collect_training_data()
Beschreibung: Startet das Sammeln von Trainingsdaten.
\end{verbatim}

\subsubsection{main - Python}
\begin{verbatim}
if __name__ == '__main__'
Beschreibung: Initialisiert den Paketpfad und startet das Sammeln 
von Trainingsdaten.
\end{verbatim}

\subsubsection{plot\_confusion\_matrix - Python}
\begin{verbatim}
def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix',
cmap=plt.cm.Blues)
Beschreibung: Erstellt eine Konfusionsmatrix und stellt diese graphisch dar. 
@param cm: Die echten Labels.
@param classes: Die vorhergesagten Labels.
\end{verbatim}

\subsubsection{main - Python}
\begin{verbatim}
if __name__ == '__main__'
Beschreibung: Lädt die Trainingsdaten und startet das Trainieren der SVM, 
welche anschließend gespeichert wird.
\end{verbatim}

\subsection{Schnittstellen}
\chapterauthor{Alexander Haar}

\subsubsection{Service Server classify\_service}
\begin{verbatim}
def classify(req)
Beschreibung: Service- Methode, welche einen Featurevektor klassifiziert.
@param req: Die Anfrage an den Service, welche den Featurevektor enhält.  
@param res: Die Antwort des Services, welche das Label enthält.
\end{verbatim}

\subsection{Programmablauf}
\chapterauthor{Alexander Haar}
\subsubsection{Schritt 1: Trainingsdaten sammeln und speichern}
Es werden aus csv- Dateien die Featurevektoren mit Labels ausgelesen und als sav- Datei gespeichert.

\subsubsection{Schritt 2: Trainieren und speichern einer SVM} 
Die Trainingsdaten werden geladen, danach wird eine SVM mit den Daten trainiert und gespeichert.

\subsubsection{Schritt 3: Laden der SVM und Klassifizieren von Featurevektoren}
Die beiden vorherigen Schritte werden nur einmal im Vorfeld ausgeführt. Das Hauptprogramm lädt die SVM und startet anschließend den Classify- Service.

\section{beliefstate}
\chapterauthor{Alexander Haar}
Das beliefstate-Paket dient dazu sich Aktionen die der Roboter ausgeführt hat zu speichern und auf Anfrage zu ermitteln, welcher Gripper belegt ist und welche Objekte als nächstes weggeräumt werden sollen. Außerdem spawnt der Knoten die Objektkoordinatensysteme und spawnt außerdem das Mesh des Objekts.

\begin{figure}[!htb]
        \center{\includegraphics[width=\textwidth]
        {figures/beliefstate.png}
        \caption{\label{fig:beliefstate_node} Architektur der beliefstate-node}}
\end{figure}
      
\subsection{Beschreibung des Teilsystems}
\chapterauthor{Alexander Haar}

\subsubsection{to\_underscore - Python}
\begin{verbatim}
def to_underscore(name)
Beschreibung: Übersetzt einen string von Camelcase- Schreibweise 
in underscore- Schreibweise. 
@param name: Der string in Camelcase- Schreibweise.
@return: Der string in underscore- Schreibweise.
\end{verbatim}

\subsubsection{object\_url\_to\_object\_name - Python}
\begin{verbatim}
def object_url_to_object_name(url)
Beschreibung: Extrahiert aus einer Owl-Url den Namen eines Objekts.
@param url: Die Owl- Url.
@return: Der Objektname.
\end{verbatim}


\subsubsection{prolog\_query\_false - Python}
\begin{verbatim}
def prolog_query_false(query_result)
Beschreibung: Evaluiert, ob eine Prologanfrage zu false auswertet.
@param query_result: Resultat einer einer Prologanfrage.
@return: True, falls die Prologanfrage zu false evaluiert und false sonst.
\end{verbatim}

\subsubsection{prolog\_query\_true - Python}
\begin{verbatim}
def prolog_query_true(query_result)
Beschreibung: Evaluiert, ob eine Prologanfrage zu true auswertet.
@param query_result: Resultat einer einer Prologanfrage.
@return: True, falls die Prologanfrage zu true evaluiert und false sonst.
\end{verbatim}

\subsubsection{point\_to\_prolog\_list - Python}
\begin{verbatim}
def point_to_prolog_list(point)
Beschreibung: Übersetzt einen Punkt in eine Prologliste.
@param point: Der zu übersetzende Punkt.
@return: Die resultierende Prologliste.
\end{verbatim}

\subsubsection{pose\_to\_prolog\_list - Python}
\begin{verbatim}
def pose_to_prolog_list(pose)
Beschreibung: Übersetzt eine Pose in eine Prologliste.
@param pose: Die zu übersetzende Pose.
@return: Die resultierende Prologliste.
\end{verbatim}

\subsubsection{gripper\_as\_string - Python}
\begin{verbatim}
def gripper_as_string(gripper)
Beschreibung: Übersetzt eine Gripperkonstante in einen string.
@param gripper: Die zu übersetzende Gripperkonstante.
@return: Der resultierende string.
\end{verbatim}

\subsubsection{create\_query\_for\_perceive\_object - Python}
\begin{verbatim}
def create_query_for_perceive_object(perceive_object_msg)
Beschreibung: Erzeugt anhand einer PerceiveObject.msg eine Prologanfrage.
@param perceive_object_msg: Die PerceiveObject.msg.
@return: Die resultierende Prologanfrage.
\end{verbatim}

\subsubsection{create\_query\_for\_grasp\_object - Python}
\begin{verbatim}
def create_query_for_grasp_object(grasp_object_msg)
Beschreibung: Erzeugt anhand einer GraspObject.msg eine Prologanfrage.
@param grasp_object_msg: Die GraspObject.msg.
@return: Die resultierende Prologanfrage.
\end{verbatim}

\subsubsection{create\_query\_for\_drop\_object - Python}
\begin{verbatim}
def create_query_for_drop_object(drop_object_msg)
Beschreibung: Erzeugt anhand einer DropObject.msg eine Prologanfrage.
@param drop_object_msg: Die DropObject.msg.
@return: Die resultierende Prologanfrage.
\end{verbatim}

\subsubsection{create\_query\_object\_attached\_to\_gripper - Python}
\begin{verbatim}
def create_query_object_attached_to_gripper(gripper)
Beschreibung: Erzeugt anhand einer Gripperkonstante eine Prologanfrage.
@param gripper: Die Gripperkonstante.
@return: Die resultierende Prologanfrage.
\end{verbatim}

\subsubsection{object\_exists - Python}
\begin{verbatim}
def object_exists(object_class)
Beschreibung: Evaluiert, ob ein Objekt bereits wahrgenommen wurde und in 
der Wissensbasis existiert.
@param object_class: Das zu überprüfende Objekt.
@return: True, falls das Objekt in der Wissensbasis existiert und false sonst.
\end{verbatim}

\subsubsection{pose\_stamped\_to\_position\_tupel - Python}
\begin{verbatim}
def pose_stamped_to_position_tupel(pose_stamped)
Beschreibung: Übersetzt die Position eines PoseStamped in einen Tupel.
@param pose_stamped: Der zu übersetzende PoseStamped.
@return: Das resultierende Tupel.
\end{verbatim}

\subsubsection{pose\_stamped\_to\_quaternion\_tupel - Python}
\begin{verbatim}
def pose_stamped_to_quaternion_tupel(pose_stamped)
Beschreibung: Übersetzt das Quaternion eines PoseStamped in ein Tupel.
@param pose_stamped: Der zu übersetzende PoseStamped.
@return: Das resultierende Tupel.
\end{verbatim}

\subsubsection{quaternion\_to\_prolog\_list - Python}
\begin{verbatim}
def quaternion_to_prolog_list(quaternion)
Beschreibung: Übersetzt ein Quaternion in eine Prologliste.
@param quaternion: Das zu übersetzende Quaternion.
@return: Die resultierende Prologliste.
\end{verbatim}

\subsubsection{spawn\_object\_frame - Python}
\begin{verbatim}
def spawn_object_frame(object_name, object_pose)
Beschreibung: Spawnt an der angegebenen Pose ein Objektkoordinatensystem 
und das entsprechende Mesh.
@param object_name: Der Name des Objekts.
@param object_pose: Die Pose des Objekts.
\end{verbatim}

\subsubsection{update\_object\_frame - Python}
\begin{verbatim}
def update_object_frame(object_name, object_pose)
Beschreibung: Ändert die Pose des angegeben Objekts auf die übergebene
              Pose.
@param object_name: Der Name des Objekts.
@param object_pose: Die Pose des Objekts.
\end{verbatim}

\subsubsection{change\_reference\_frame - Python}
\begin{verbatim}
def change_reference_frame(object_name, new_reference_frame_id)
Beschreibung: Verändert das Referenzframe des Objektkoordinatensystems 
auf das übergeben Koordinatensystem.
@param object_name: Der Name des Objekts.
@param new_reference_frame_id: Das neu Referenzkoordinatensystem.
\end{verbatim}

\subsubsection{spawn\_object\_mesh - Python}
\begin{verbatim}
def spawn_object_mesh(object_name)
Beschreibung: Spawnt ein Mesh in dem entsprechenden Objektkoordinatensystem.
@param object_name: Der Name des Objekts.
\end{verbatim}

\subsubsection{main - Python}
\begin{verbatim}
__name__ == '__main__
Beschreibung: Startet alle Service- Server und Subscriber die
dieser Knoten benötigt.
\end{verbatim}

\subsubsection{object\_exists - Prolog}
\begin{verbatim}
object_exists(ObjectClass)
Beschreibung: Prüft, ob ein Individual dieser Klasse bereits in der
Wissensbasis vorhanden ist
@param ObjectClass: Die zu prüfende Objektklasse.
\end{verbatim}

\subsubsection{process\_perceive\_action - Prolog}
\begin{verbatim}
process_perceive_action(ObjectClass, PoseList, ReferenceFrame)
Beschreibung: Fügt ein Individual der Objektklasse an der angegebenen 
Pose im Referenzframe hinzu.
@param ObjectClass: Die Klasse der hinzuzufügenden Individuals.
@param PoseList: Die Pose des Objekts als Liste.
@param ReferenceFrame: Das Referenzframe.
\end{verbatim}

\subsubsection{process\_grasp\_action - Prolog}
\begin{verbatim}
process_grasp_action(ObjectClass, GripperIndividual)
Beschreibung: Fügt eine GraspAction zur Wissensbasis hinzu unter 
Verwendung des angegeben Grippers und der Objektklasse.
@param ObjectClass: Die Objektklasse.
@param GripperIndividual: Linker oder rechter Gripper.
\end{verbatim}

\subsubsection{process\_drop\_action - Prolog}
\begin{verbatim}
process_drop_action(GripperIndividual)
Beschreibung: Fügt eine DropAction zur Wissensbasis hinzu unter 
Verwendung des angegeben Grippers.
@param GripperIndividual: Linker oder rechter Gripper.
\end{verbatim}

\subsubsection{object\_attached\_to\_gripper - Prolog}
\begin{verbatim}
object_attached_to_gripper(GripperIndividual, ObjectIndividual)
Beschreibung: Prüft, ob sich ein Objekt in dem angegebenen Gripper befindet.
@param GripperIndividual: Linker oder rechter Gripper.
@param ObjectIndividual: Das Objekt, welches sich im Gripper befindet.
\end{verbatim}

\subsubsection{get\_latest\_object\_pose - Prolog}
\begin{verbatim}
get_latest_object_pose(ObjectIndividual, PoseList)
Beschreibung: Gibt die zuletzt bekannte Pose des Objekts zurück.
@param ObjectIndividual: Das Objekt.
@param PoseList: Die Pose des Objekts.
\end{verbatim}

\subsubsection{get\_objects\_on\_kitchen\_island\_counter - Prolog}
\begin{verbatim}
get_objects_on_kitchen_island_counter(ObjectList)
Beschreibung: Gibt eine Liste aller Objekte die noch verteilt
werden müssen zurück.
@param ObjectList: Eine Liste aller Objekte die noch verteilt werden müssen.
\end{verbatim}

\subsubsection{get\_two\_objects\_on\_kitchen\_island\_counter\_with\_same\_storage\_place - Prolog}
\begin{verbatim}
get_two_objects_on_kitchen_island_counter_with_same_storage_place
(Object1, Object2)
Beschreibung: Gibt die zwei Objekte zurück, 
welche als nächstes verteilt werden sollen.
@param Object1: Das erste Objekt.
@param Object2: Das zweite Objekt.
\end{verbatim}

\subsection*{Schnittstellen}
\chapterauthor{Alexander Haar}

\subsubsection{Subscriber process\_perceive\_action}
\begin{verbatim}
def process_perceive_action(perceive_object_msg)
Beschreibung: Erstellt anhand der Message eine Prologanfrage und
sendet diese an den json_prolog- Server. 
@param perceive_object_msg: Die PerceiveObject.msg
\end{verbatim}

\subsubsection{Subscriber process\_grasp\_action}
\begin{verbatim}
def process_grasp_action(grasp_object_msg)
Beschreibung: Erstellt anhand der Message eine Prologanfrage und
sendet diese an den json_prolog- Server. 
@param grasp_object_msg: Die GraspObject.msg
\end{verbatim}

\subsubsection{Subscriber process\_drop\_action}
\begin{verbatim}
def process_drop_action(drop_object_msg)
Beschreibung: Erstellt anhand der Message eine Prologanfrage und
sendet diese an den json_prolog- Server. 
@param drop_object_msg: Die DropObject.msg
\end{verbatim}

\subsubsection{Service Server gripper\_empty}
\begin{verbatim}
def gripper_empty(req)
Beschreibung: Die Anfrage an den Server ist leer. Zurückgegeben werden
zwei booleans, welche angeben, ob der Gripper belegt ist.
@param req: Die Anfrage an den Service.
@return: Die Antwort des Services.
\end{verbatim}

\subsubsection{Service Server object\_to\_pick}
\begin{verbatim}
def objects_to_pick(req)
Beschreibung: Gibt die zwei Objekte zurück, welche als nächstes 
              weggeräumt werden sollen.
@param req: Die Anfrage an den Service.
@return: Die Antwort des Services.
\end{verbatim}

\subsection*{Programmablauf}
\chapterauthor{Alexander Haar}
\subsubsection{Schritt 1: Wahrnehmen mehrerer Objekte}
Es werden mehre Objekte wahrgenommen und deren Label sowie Pose wird dem beliefstate bekannt gemacht.

\subsubsection{Schritt 2: Welche zwei Objekte sollen weggeräumt werden?}
Nachdem alle zu verteilenden Objekte bekannt sind, werden möglichst zwei Objekte ausgewählt, welche den gleichen Lagerplatz besitzen.

\subsubsection{Schritt 3: Greifen eines Objekts}
Das erste der beiden Objekte wird gegriffen.

\subsubsection{Schritt 4: Greifen eines weiteren Objekts}
Anschließend wird das zweite Objekt gegriffen.

\subsubsection{Schritt 5: Abstellen des 1. Objekts}
Der Roboter fährt zum entsprechenden Lagerplatz und stellt es dort ab.

\subsubsection{Schritt 6: Anfrage welcher der Gripper belegt ist}
Nun wird geprüft, ob beide Gripper leer sind, dies ist nicht der Fall.

\subsubsection{Schritt 7: Abstellen des 2. Objekts}
Der Roboter fährt zum nächsten Lagerplatz und stellt das zweite Objekt dort ab.

\subsubsection{Schritt 8: Anfrage welcher der Gripper belegt ist}
Erneut wird geprüft, ob beide Gripper leer sind, dies ist jetzt der Fall.

\subsubsection{Schritt 9: Welche Objekte sollen weggeräumt werden?}
Nun beginnt dieser Prozess wieder bei Schritt 2.


\section{storage\_place}
\chapterauthor{Alexander Haar}
Das storage\_place-Paket dient zur Ermittlung des Lagerplatzes eines Objekts.

\begin{figure}[!htb]
        \center{\includegraphics[width=\textwidth]
        {figures/storage_place.png}
        \caption{\label{fig:vision_node} Architektur der storage\_place-node}}
\end{figure}
      
\subsection{Beschreibung des Teilsystems}
\chapterauthor{Alexander Haar}

\subsubsection{createQuery - C++}
\begin{verbatim}
std::string createQuery(std::string object_label)
Beschreibung: Erzeugt eine Prologanfrage, um einen Lagerplatz 
              zu ermitteln.
@param object_label: Der Name des Objekts von dem der Lagerplatz 
                     bestimmt werden soll.
\end{verbatim}\label{func:segmentplanes}

\subsubsection{main - C++}
\begin{verbatim}
int main(int argc, char **argv)
Beschreibung: Startet den Knoten.
\end{verbatim}\label{func:findcentergazebo}


\subsubsection{get\_position - Prolog}
\begin{verbatim}
get_position(PositionIndividual, X, Y, Z)
Beschreibung: Extrahiert aus einem Position, die Koordinaten.
@param PositionIndividual: Die Position
@param X: Die X- Koordinate
@param Y: Die Y- Koordinate
@param Z: Die Z- Koordinate
\end{verbatim}\label{func:findposes}

\subsubsection{get\_scale - Prolog}
\begin{verbatim}
get_scale(StorageAreaIndividual, Width, Height)
Beschreibung: Extrahiert aus einer Lagerfläche die Ausmaße.
@param StorageAreaIndividual: Die Lagerfläche
@param Width: Die Breite der Lagerfläche
@param Height: Die Höhe der Lagerfläche
\end{verbatim}\label{func:estimatesurfacenormals}

\subsubsection{storage\_area - Prolog}
\begin{verbatim}
storage_area(ObjectClass, StorageAreaIndividual)
Beschreibung: Ermittelt zu einer Objektklasse die Lagerfläche.
@param ObjectClass: Die Objektklasse zu der eine Lagerfläche
ermittelt werden soll.
@param StorageAreaIndividual: Die ermittelte Lagerfläche.
\end{verbatim}\label{func:apply3dfilter}

\subsubsection{storage\_place - Prolog}
\begin{verbatim}
storage_place(ObjectClass, [X, Y, Z], Width, Height)
Beschreibung: Ermittelt zu einer Objektklasse den Lagerplatz.
@param ObjectClass: Die Objektklasse zu der
ein Lagerplatz ermittelt werden soll.
@param X: Die X- Koordinate des Lagerplatzes.
@param Y: Die Y- Koordinate des Lagerplatzes.
@param Z: Die Z- Koordinate des Lagerplatzes.
@param Width: Die Breite des Lagerplatzes.
@param Height: Die Höhe des Lagerplatzes.
\end{verbatim}\label{func:estimateplaneindices}

\subsection{Schnittstellen}
\chapterauthor{Alexander Haar}

\subsubsection{Service Server find\_storage\_place}
\begin{verbatim}
bool find_storage_place(knowledge_msgs::StoragePlace::Request &req,
 knowledge_msgs::StoragePlace::Response &res)
Beschreibung: Service- Methode, welche einen Lagerplatz für Objekte 
              ermittelt.
@param req: Die Anfrage an den Service, welche den Namen des Objekts 
            enthält.  
@param res: Die Antwort des Services, welche das Zentrum und die 
            Ausmaße des Lagerplatzes enthält.
@return: true, falls der Aufruf erfolgreich war und false sonst.
\end{verbatim}\label{func:findcluster}

\subsection{Programmablauf}
\chapterauthor{Alexander Haar}
\subsubsection{Schritt 1: Anfrage an den Service- Server}
Eine Objektklasse wird an den Service- Server übergeben.
\subsubsection{Schritt 2: Erstellen der Prologanfrage}
Anhand der Objektklasse wird eine Prologanfrage erstellt und an den json\_prolog- Server geschickt.
\subsubsection{Schritt 3: Ermitteln des Lagerplatzes} 
Durch die oben definierten Prologregeln wird unter Verwendung von only- restrictions der Lagerplatz ermittelt.
\subsubsection{Schritt 4: Zurückgeben der Antwort}
Die vom json\_prolog- Server erhaltene Antwort wird entsprechend geparst und zurückgegeben.

\newpage

\section*{Methodendokumentation Gruppe Motion}

\subsection{Übersicht des Teilsystems}
\chapterauthor{Roman Haak}
Die Untergruppe '\textit{Motion}' ist für das Bewegen verschiedener "Körperpartien" des Roboters zwecks Manipulation der Umwelt zuständig.\\
Lediglich das Bewegen der Basis des Roboters wird von der Gruppe \textit{Planning} übernommen.\\
Das Bewegen der anderen, zum Erfüllen der Zielsetzung benötigten Teile des Roboters, also Arm und Endeffektor (\textit{Gripper}) werden 
von der Gruppe '\textit{Motion}' übernommen.\\
Dabei realisiert \textit{Motion} im Wesentlichen das Greifen, Abstellen und Verschieben von Objekten.

In \textit{Abbildung 1} Bild sieht man die Architektur des Teilsystems der Gruppe '\textit{Motion}'. Dabei gibt es zwei Knoten '\textit{suturo\_motion\_main}' und '\textit{simple\_gripper}'.\\

\begin{figure}[!htb]
        \center{\includegraphics[width=\textwidth]
        {img/Architekturbild.png}
        \caption{\label{fig:motion_node} Architektur des Teilsystems der Gruppe 'Motion'}}
\end{figure}

Der Knoten '\textit{suturo\_motion\_main}' ist für das Verarbeiten eingehender Kommandos zum Bewegen der Arme bzw. eines Endeffektors zuständig. Dabei gibt es Kommandos für das Fahren der Arme in eine initiale Position oder eine Position, welche nach dem Greifen eines Objektes eingenommen wird. Zusätzlich Kommandos für das Bewegen der Arme zu einer bestimmten Pose oder das Umstubsen, Greifen oder Abstellen eines bestimmten Objektes mit einer gegebenen Pose.\\
Der Knoten '\textit{simple\_gripper}' wird dabei benutzt, um das Öffnen und Schließen des Endeffektors zu realisieren. Dieser Knoten wird von dem Hauptknoten '\textit{suturo\_motion\_main}' über einen Actionserver aufgerufen.\\

Im Folgenden werden die \textit{Schnittstellen}, der \textit{Ablauf} des Teilsystems und besonders \textit{hervorzuhebende} \textit{Algorithmen} beschrieben.\\

\subsection{Schnittstellen - 'suturo\_motion\_main'}
\subsubsection{Angebotene Actions}
\textbf{MovingCommand}
\chapterauthor{Maximilian Bertram}
Vom Typ '\textit{motion\_msgs/MovingCommand}': \\

\subsubsection*{Definition des MovingCommands}
\begin{verbatim}
# goal definition
# goal_pose is used when an action, which is not the grasp action is called
# else goal_poses is used
geometry_msgs/PoseStamped goal_pose
geometry_msgs/PoseArray goal_poses

uint8 command
#Constants for command value
uint8 UNKNOWN=0
uint8 MOVE_DRIVE_POSE=1
uint8 MOVE_RIGHT_ARM=2
uint8 MOVE_LEFT_ARM=3
uint8 POKE_RIGHT_ARM=4
uint8 POKE_LEFT_ARM=5
uint8 GRASP_RIGHT_ARM=6
uint8 GRASP_LEFT_ARM=7
uint8 PLACE_RIGHT_ARM=8
uint8 PLACE_LEFT_ARM=9
uint8 MOVE_CARRY_POSE=10
uint8 MOVE_CARRY_POSE_RIGHT=11
uint8 MOVE_CARRY_POSE_LEFT=12

#The force when closing the gripper in newton default is set below
float64 force
float64 FORCE_DEFAULT=35

# Label of object which shell be grasped, placed or poked.
# Required because when an object gets grasped by robot,
# a message gets published from subgroup 'motion', which object
# has been grasped with which gripper (left or right).
# When the object gets placed somewhere, another message is published.
# These messages get published for knowledge's beliefstate, so that
# the knowledge component knows when an object get's grasped/placed somewhere.
# Additionaly required for some planningscene operations when grasping, placing or poking an object.
string grasped_object_label

---
#result definition
bool successful
uint8 status
#Constants for status value
uint8 SUCCESS=0
uint8 OUT_OF_RANGE=1
uint8 COLLISION=2
uint8 UNMANAGEBLE_ERROR=3

---
#feedback definition
#bool finished
\end{verbatim}

Mit Hilfe der MovingCommandAction lassen sich alle von uns zur Verfügung gestellten Funktionen aufrufen. Es wurden Konstanten definiert, die die jeweilige Aktion beschreiben und in der Message-Definition vorhanden sind.

\subsubsection{Aufgerufene Actions}
\chapterauthor{Maximilian Bertram}
Wird eine GRASP oder PLACE Aktion ausgeführt, wird über eine \\
\textit{motion\_msgs::Gripper}-Message an den Actionserver des Knotens '\textit{simple\_gripper}' der Gripper entsprechend geöffnet oder geschlossen.(Siehe !!!!-- TODO --!!!! Verweis auf Beschreibung zu unserer simple-Gripper-Node)\\


\subsubsection{Aufgerufene Services}
\chapterauthor{RH, MB}
\textbf{GetFixedKitchenObjects}\\
Vom Typ '\textit{knowledge\_msgs/GetFixedKitchenObjects}': \\ 
\begin{verbatim}
#This servicedefinition is for transfering the shape of the objects in the iai_kitchen.
#The Motion-component calls this service for getting the shape of the kitchen-objects for it's planningscene.
#The Knowledge-component reads these shapes out of an owl-file and returns the data described below.

---

#The names of the kitchen objects
string[] names
#The path to the meshes of the kitchen objects
string[] meshes
#The names of the tf-frames of the kitchen objects. The kitchen object meshes get spawned
#at the position of the corresponding frames.
string[] frames
\end{verbatim}

\subsubsection{Angebotene Topics}
\textbf{GraspAction und DropAction}
\chapterauthor{Maximilian Bertram}

Wird ein Objekt erfolgreich abgestellt oder gegriffen werden für den \textit{Beliefstate} von \textit{Knowledge} noch die entsprechenden \textit{Messages} gepublished: \\

\subsubsection*{Definition der GraspObject.msg}
\begin{verbatim}
#A message which get's published by Motion-component after grasping an object.

#The label of the grasped object.
string object_label

#The gripper used. See 'Gripper.msg' for more details.
Gripper gripper
\end{verbatim}

\subsubsection*{Definition der DropObject.msg}
\begin{verbatim}
#A message which get's published by Motion-component after dropping an object.

#The gripper used. See 'Gripper.msg' for more details.
Gripper gripper
\end{verbatim}

\subsubsection*{Definition der Gripper.msg}
\begin{verbatim}
#A message for defining constants for differencing between left and right gripper.

uint8 gripper
#constants for gripper
uint8 LEFT_GRIPPER = 1
uint8 RIGHT_GRIPPER = 2
\end{verbatim}

\subsubsection{Benutzte Topics}
\textbf{PerceivedObjectBoundingBox}
\chapterauthor{Roman Haak}

Wird ein Objekt wahrgenommen, werden Daten über das Objekt auf dem Topic veröffentlicht. Diese werden genutzt, um das wahrgenommen Objekt zu unserer Planningscene hinzuzufügen.\\ Dabei sind folgende Daten über das Objekt in der Nachricht enthalten: \\

\subsubsection*{Definition der PerceivedObjectBoundingBox.msg}
\begin{verbatim}
#A message which get's published by knowledge when an object is perceived.

#The name of the object.
string object_label
#The path to the mesh of the object.
string mesh_path
#The pose the perceived object has.
geometry_msgs/PoseStamped pose
\end{verbatim}


\textbf{SpawnAttachedObject}
\chapterauthor{Roman Haak}

Kann ein Objekt nicht durch den Roboter gegriffen werden, wird der Mensch um Hilfe gefragt. Hat der Mensch dem Roboter das Objekt in den Gripper gegeben und wurde der Gripper geschloßen, werden Daten über den Vorgang über dieses Topic veröffentlicht. Diese werden genutzt, um das 'gegriffene' Objekt zu unserer Planningscene hinzuzufügen und an den entsprechenden Gripper des Roboters anzuheften.\\ Dabei sind folgende Daten über den Vorgang in der Nachricht enthalten: \\

\subsubsection*{Definition der SpawnAttachedObject.msg}
\begin{verbatim}
#A message which get's published by knowledge when an object is put in the gripper of the robot by a human.
#In which gripper the object was put can be determined by looking in the header's frame id. If the pose is given in 
#the 'l_gripper_tool_frame', the object is in the left gripper and in the right gripper, if 'r_gripper_tool_frame' is
#the referenced frame.

#The pose of the object. 
geometry_msgs/PoseStamped pose
#The name of the object.
string object_label
\end{verbatim}



\textbf{DeleteObjectHumanInteraction}
\chapterauthor{Roman Haak}

Kann ein Objekt nicht durch den Roboter abgestellt werden, wird der Mensch um Hilfe gefragt. Hat der Mensch dem Roboter das Objekt aus dem Gripper genommen, werden Daten über den Vorgang über dieses Topic veröffentlicht. Diese werden genutzt, um das 'abgestellte' Objekt aus unserer Planningscene zu entfernen, weil es für die Bewegungsplanung ab diesem Zeitpunkt nicht mehr relevant ist.\\ Dabei besteht die Nachricht lediglich aus einem String, der den Namen des entfernten Objektes enthält.




\subsection{Schnittstellen - 'simple\_gripper'}
\chapterauthor{Maximilian Bertram}
\subsubsection{Angebotene Actions}
Vom Typ '\textit{motion\_msgs/Gripper}': \\
\subsubsection*{Definition der GripperAction}
\begin{verbatim}
#goal definition
#position of the space between the grippers in meters
float64 position
#force in newton (default =  20)
float64 effort
#which grpper to open/closed
uint8 gripper
#Constants for the gripper value
uint8 LEFT_GRIPPER=1
uint8 RIGHT_GRIPPER=2
---
#result definition
bool successful
---
#feedback definition
#bool finished
\end{verbatim}
\subsubsection{Aufgerufene Actions}
Die GripperAction-Message wird entsprechend geparsed und eine \\
 \textit{pr2\_ controllers\_ msgs::Pr2GripperCommandAction} erzeugt und gepublished. \\

\subsection{Ablauf}
\chapterauthor{Maximilian Bertram, Roman Haak}
Der generelle Ablauf unseres Programms ist im Folgenden Ablaufdiagramm dargestellt.


\begin{figure}[!htb]
        \center{\includegraphics[width=\textwidth]
        {img/Ablaufdiagramm.png}
        \caption{\label{fig:motion_node_flow_chart} Ablaufdiagramm 'Motion'}}
\end{figure}




\subsection{Besonderheiten/Besondere Algorithmen}
In diesem Abschnitt werden kurz erwähnenswerte Algorithmen beschrieben.
Dabei wird zuerst der Algorithmus für das Umstoßen eines Objektes und anschließend der Algorithmus für das Greifen, bzw. Abstellen eines Objektes beschrieben.\\

\subsubsection{Algorithmus für das Umstoßen eines Objektes}
\chapterauthor{Roman Haak}
Über die von unserem Teilsystem angebotene Action '\textit{moving}' kann das Kommando zum Umstoßen eines Objektes mit dem rechten oder mit dem linken \textit{Gripper} abgesetzt werden. Beim Aufruf wird dabei eine Pose übergeben, welche die Position des Mittelpunktes des umzustoßenden Objektes und die beabsichtigte Orientierung des \textit{Grippers} beinhaltet. \\Der Algorithmus, der das Umstoßen dann realisiert, arbeitet wie folgt:\\

1. \textit{Berechne die Pose, zu der der Arm als erstes gefahren werden soll. Diese Pose wird folgendermaßen ermittelt:}\\
\tab - die Orientierung wird von der beim Aufruf des Kommandos übergebenen Pose übernommen\\
\tab - die Position ist die Position des Objektmittelpunkts, von der noch die Distanz\\ \tab   'DISTANCE\_BEFORE\_POKING' und 'GRIPPER\_LENGTH\_LEFT'\\ \tab bzw. 'GRIPPER\_LENGTH\_RIGHT' von der X-Koordinate im 'base\_footprint'-Frame \\ \tab abgezogen wird, wobei 'DISTANCE\_BEFORE\_POKING', 'GRIPPER\_LENGTH\_LEFT' \tab und 'GRIPPER\_LENGTH\_RIGHT' definierte Konstanten sind.\\

Der so errechnete Zielpunkt befindet sich also 'DISTANCE\_BEFORE\_POKING' plus 'GRIPPER\_LENGTH\_LEFT' bzw. 'GRIPPER\_LENGTH\_RIGHT' vor dem Objekt, aus Sicht des Roboters.\\

2. \textit{Fahre mit dem rechten bzw. linken Arm zu der errechneten Pose.}\\

3. \textit{Errechne nun einen Punkt, der etwas über dem Objektmittelpunkt liegt. Falls der linke Arm benutzt wird, verschiebe den Punkt zusätzlich noch um die Differenz zwischen 'GRIPPER\_LENGTH\_LEFT' und 'GRIPPER\_LENGTH\_RIGHT' hin zum Roboter. Damit soll erreicht werden, dass immer auf dieselbe Weise umgestoßen wird, egal ob der linke (längere) oder rechte Arm benutzt wird.}\\

4. \textit{Berrechne eine 10-Punkte Trajektorie zwischen dem in Schritt 3 errechneten Punkt und der aktuellen Position des Armes.}\\
\tab - Dies hat den Sinn, dass sichergestellt ist, dass das Objekt von\\ \tab vorne umgestoßen wird.\\

5. \textit{Fahre den Arm entlang der gewünschten Trajektorie.}\\
\tab - Das Objekt sollte nun umgestoßen worden sein.


\subsubsection{Algorithmus für das Greifen/Abstellen eines Objektes}
\chapterauthor{Roman Haak}
Über die von unserem Teilsystem angebotene Action '\textit{moving}' kann das Kommando zum Greifen/Abstellen eines Objektes mit dem rechten oder mit dem linken \textit{Gripper} abgesetzt werden. Beim Aufruf wird dabei eine Pose übergeben, welche die Position des Greifpunktes des zu greifenden Objektes, bzw. die Position, zu der der \textit{Gripper} hingefahren werden soll, um das Objekt abzustellen beinhaltet. Zusätzlich ist noch die beabsichtigte Orientierung des \textit{Grippers} angegeben. \\Der Algorithmus, der das Greifen bzw. Abstellen dann realisiert, ist dabei derselbe. Denn die beiden Aktionen Greifen und Abstellen unterscheiden sich prinzipiell nur daran, dass beim Greifen der \textit{Gripper} erst geöffnet und dann zum Greifen geschloßen wird und beim Abstellen der \textit{Gripper} zuerst bereits geschloßen ist und beim Abstellen des Objektes dann geöffnet werden muss. Zu dieser Unterscheidung reichen also Fallunterscheidungen innerhalb des Algorithmus. \\Der Algorithmus arbeitet dabei wie folgt:\\

1. \textit{Hebe den gewünschten Arm auf die Höhe 'TABLE\_HEIGHT' + 'MAXIMUM\_OBJECT\_HEIGHT' + 'GRIPPER\_LENGTH\_LEFT' + 'DISTANCE\_BEFORE\_GRASPING'.}\\
\tab - Da wir es in unserem aktuellen Stand des Gesamtsystems noch nicht geschafft haben, \\ \tab beim Greifen die Kollisionsfreiheit sicherzustellen, haben wir uns entschieden das Greifen\\ \tab aus einer Höhe zu realisieren, auf der sich sicher kein Objekt befindet.\\

2. \textit{Bewege den Arm auf dieser Höhe über das zu greifende/abzustellende Objekt.}\\

3. \textit{Ändere die Orientierung des Armes zu der Orientierung, die in der übergebenen Pose beinhaltet ist.}\\
\tab - Die übergebene Orientierung ist in dem aktuellen Stand die, bei der der \textit{Gripper} nach \\ \tab unten orientiert ist, damit das Objekt so gegriffen werden kann.\\

4. \textit{Hier findet eine Fallunterscheidung statt.}\\
\tab - Soll ein Objekt gegriffen werden, öffne den \textit{Gripper}.\\
\tab - Soll ein Objekt abgestellt werden, lass den \textit{Gripper}, wie er ist.\\

5. \textit{Fahre mit dem Arm zu der Position, welche in der Pose angegeben ist.}\\

6. \textit{Hier findet wieder eine Fallunterscheidung statt.}\\
\tab - Soll ein Objekt gegriffen werden, schließe den \textit{Gripper}.\\
\tab - Soll ein Objekt abgestellt werden, öffne den \textit{Gripper}.\\

7. \textit{Hebe den Arm wieder auf die am Anfang errechnete Höhe an, um das Objekt zu heben, bzw. "vom Objekt wegzufahren".}\\
\tab - Das Objekt sollte nun entweder an dem gewünschten Platz stehen, bzw. sich in dem\\ \tab \textit{Gripper} des Roboters befinden.


\subsection{Tests \& Evaluation}
\chapterauthor{MB, RH}
Da wir agil entwickeln, haben wir abgewägt und die Priorität auf die Zielerreichung des 2.
Meilensteines gesetzt und uns gegen das Implementieren von Unit-Tests durch ein Testframework entschieden. Alle von Motion angebotenen Funktionen (greifen, abstellen, umstubsen) wurden mehrfach manuell am echten Roboter überprüft und getestet. Die Implementierung von Unit-Tests wird im 3. Meilenstein folgen. \\

Durch die händischen Tests sind immer wieder Differenzen zwischen echtem PR2 und der Simulation aufgefallen. Es mussten immer wieder Anpassungen am Code vorgenommen werden, damit das Package auf dem jeweiligen System wie gewünscht funktioniert. Die Fehlersuche hat hierbei enorm Zeit verbraucht. \\

Am Ende funktionierten die angebotenen Funktionalitäten wie gewünscht.\\

\newpage

\section*{Methodendokumentation Gruppe Vision}

\section{Vision}
\subsection{Vorwort Vision}
\chapterauthor{Alexander Link}
Da einige Teile der Dokumentation überarbeitet und nicht komplett neu geschrieben wurden, ist zu beachten, dass die Autoren der Vision-Teile zu der Dokumentation von Meilenstein 1 genau umgekehrt waren. Auch ist hinzuzufügen, dass an vielen Funktionen Tammo Wübbena und Alexander Link zugleich beteiligt waren. In solchen Fällen sind beide Autoren mit "Beide"\ abgekürzt.

\subsection{Überblick Vision}
\chapterauthor{Alexander Link}
Das Vision-Paket \footnote{im weiteren Verlauf auch Vision-Package, vision-node oder einfach Paket oder vision genannt} dient zur Akquirierung und Verarbeitung visueller Informationen durch die Kinect-Kamera des PR2-Roboters.

\begin{figure}[!htb]
        \center{\includegraphics[width=\textwidth]
        {figures/vision_diagram_milestone_2.png}
        \caption{\label{fig:vision_node} Architektur der vision-node}}
\end{figure}
      
\subsection{Beschreibung des Teilsystems}
\chapterauthor{Tammo Wübbena, Update: Alexander Link}

\subsubsection{Abkürzungen durch short\_types.h}
\chapterauthor{Alexander Link (Doku), Beide (Code)}
Für bessere Lesbarkeit haben wir typedef Kürzel verwendet:
\begin{verbatim}
typedef pcl::PointCloud<pcl::PointXYZ>::Ptr PointCloudXYZPtr;
typedef pcl::PointCloud<pcl::PointXYZRGB>::Ptr PointCloudRGBPtr;
typedef pcl::PointCloud<pcl::Normal>::Ptr PointCloudNormalPtr;
typedef pcl::PointCloud<pcl::PointNormal>::Ptr PointCloudPointNormalPtr;
typedef pcl::PointCloud<pcl::PointXYZ> PointCloudXYZ;
typedef pcl::PointCloud<pcl::PointXYZRGB> PointCloudRGB;
typedef pcl::PointCloud<pcl::Normal> PointCloudNormal;
typedef pcl::PointIndices::Ptr PointIndices;
typedef std::vector<pcl::PointIndices> PointIndicesVector;
typedef std::vector<pcl::PointIndices::Ptr> PointIndicesVectorPtr;

typedef geometry_msgs::PointStamped PointStamped;
typedef pcl::PointCloud<pcl::VFHSignature308>::Ptr PointCloudVFHS308Ptr;
typedef sensor_msgs::PointCloud2 SMSGSPointCloud2;
typedef std::vector<pcl::PointCloud<pcl::PointXYZ>::Ptr> PointCloudXYZPtrVector;
\end{verbatim}

\subsubsection*{Perception}

\subsubsection{preprocessCloud}
\chapterauthor{Alexander Link (Doku), Tammo Wübbena (Code)}
\begin{verbatim}
findCluster(const PointCloudXYZPtr kinect)
Beschreibung: Wendet unsere Filter auf eine Punktwolke an.
@param: kinect Punktwolke
@return: Vorprozessierte Punktwolke
\end{verbatim}\label{func:preprocesscloud}

\subsubsection{segmentPlanes}
\chapterauthor{Alexander Link (Doku), Beide (Code)}
\begin{verbatim}
void segmentPlanes(PointCloudRGBPtr cloud_cluster)
Beschreibung: Segmentiert Flächen, die nicht für die gesuchten Objekte
relevant sind.
@param: PointCloud, aus der Flächen entfernt werden sollen.
\end{verbatim}\label{func:segmentplanes}

\subsubsection{findCluster}
\chapterauthor{Alexander Link (Doku), Beide (Code)}
\begin{verbatim}
std::vector<PointCloudRGBPtr> findCluster(PointCloudRGBPtr kinect)
Beschreibung: Findet die Objekte und gibt eine Punktwolke pro Objekt zurück.
@param: Punktwolke der Kinect
@return: Punktwolken der Objekte
\end{verbatim}\label{func:findcluster}

\subsubsection{findCenterGazebo}
\chapterauthor{Alexander Link (Doku), Beide (Code)}
\begin{verbatim}
geometry_msgs::PointStamped findCenterGazebo()
Beschreibung: Gibt einen künstlichen Punkt zurück (Den Mittelpunkt des
Modells, 
das an Gazebo übergeben wurde).
@return: Künstlicher 3D-Mittelpunkt
\end{verbatim}\label{func:findcentergazebo}


\subsubsection{findPoses}
\chapterauthor{Alexander Link (Doku), Beide (Code)}
\begin{verbatim}
std::vector<geometry_msgs::PoseStamped> 
findPoses(const std::vector<PointCloudRGBPtr> clouds_in)
Beschreibung: Errechnet Position und Rotation der Objekte in den übergebenen
Punktwolken.
@param: Punktwolken der Objekte
@return: Posen der Objekte
\end{verbatim}\label{func:findposes}

\subsubsection{estimateSurfaceNormals}
\chapterauthor{Alexander Link (Doku), Beide (Code)}
\begin{verbatim}
PointCloudNormalPtr estimateSurfaceNormals(PointCloudRGBPtr input)
Beschreibung: Schätzt die Normalen einer Punktwolke.
@param: Punktwolke, zu der die Normalen berechnet werden sollen
@return: Normalen der übergebenen Punktwolke
\end{verbatim}\label{func:estimatesurfacenormals}

\subsubsection{apply3DFilter}
\chapterauthor{Alexander Link (Doku), Beide (Code)}
\begin{verbatim}
PointCloudRGBPtr apply3DFilter(PointCloudRGBPtr input, float x, float y, 
float z)
Beschreibung: Filtert eine Punktwolke, indem sie nur die Punkte in einem
bestimmten Bereich ((-x,x), (-y,y), (0.0,z)) erhält.
@param: Punktwolke, die gefiltert werden soll
		x,y,z Bereiche der Achsen für den Filter
@return Gefilterte Punktwolke
\end{verbatim}\label{func:apply3dfilter}

\subsubsection{estimatePlaneIndices}
\chapterauthor{Alexander Link (Doku), Beide (Code)}
\begin{verbatim}
PointIndices estimatePlaneIndices(PointCloudRGBPtr input)
Beschreibung: Errechnet die Indizes einer Fläche einer Punktwolke.
@param: Punktwolke, dessen Fläche ermittelt werden soll
@return Indizes der Fläche, wenn eine gefunden wurde
\end{verbatim}\label{func:estimateplaneindices}

\subsubsection{extractCluster}
\chapterauthor{Alexander Link (Doku), Beide (Code)}
\begin{verbatim}
PointCloudRGBPtr extractCluster(PointCloudRGBPtr input, PointIndices indices, 
bool negative);
Beschreibung: Extrahiert Objekt-Cluster anhand übergebener Punktwolke und
Indizes.
@param: input Punktwolke, aus der ein Cluster extrahiert werden soll
		indices Indizes der Punkte in input, die extrahiert werden sollen
		negative Entscheidet, ob Indizes extrahiert werden sollen (false) oder
		alle Punkte, außer den in Indizes angegebenen (true).
@return Objekt-Cluster Punktwolke
\end{verbatim}\label{func:extractcluster}

\subsubsection{mlsFilter}
\chapterauthor{Alexander Link (Doku), Beide (Code)}
\begin{verbatim}
PointCloudRGBPtr mlsFilter(PointCloudXYZPtr input);
Beschreibung: Glättet Punktwolke unter Verwendung eines Moving-Least-Squares
Algorithmus.
@param: Punktwolke, die mittels MLS-Algorithmus geglättet wird
@return Geglättete Punktwolke
\end{verbatim}\label{func:mlsfilter}

\subsubsection{voxelGridFilter}
\chapterauthor{Alexander Link (Doku), Beide (Code)}
\begin{verbatim}
PointCloudRGBPtr voxelGridFilter(PointCloudRGBPtr input)
Beschreibung: Filtert eine Punktwolke mit dem Voxel-Gitter-Filter.
@param: Zu filternde Punktwolke
@return Mit einem Voxel-Gitter gefilterterte (und damit gedownsamplete)
Punktwolke
\end{verbatim}\label{func:voxelgridfilter}

\subsubsection{outlierRemoval}
\chapterauthor{Alexander Link (Doku), Beide (Code)}
\begin{verbatim}
PointCloudRGBPtr outlierRemoval(PointCloudRGBPtr input)
Beschreibung: Entfernt Punkte aus Punktwolke, die als zu weit außen liegend
(outlier) oder als Rauschen erkannt werden.
@param: Punktwolke, die von Rauschen befreit werden soll
@return Gefilterte Punktwolke
\end{verbatim}\label{func:outlierremoval}

\subsubsection{cvfhRecognition}
\chapterauthor{Alexander Link (Doku + Code)}
\begin{verbatim}
PointCloudVFHS308Ptr cvfhRecognition(PointCloudRGBPtr input)
Beschreibung: Errechnet features eines Objekts in einer Punktwolke als
VFHSignature308
@param: Punktwolke, dessen features berechnet werden sollen
@return Features als VFHSignature308
\end{verbatim}\label{func:cvfhRecognition}

\subsubsection{euclideanClusterExtraction}
\chapterauthor{Alexander Link (Doku + Code)}
\begin{verbatim}
std::vector<PointCloudRGBPtr> euclideanClusterExtraction(PointCloudRGBPtr
input)
Beschreibung: Trennt cluster einer Punktwolke voneinander.
@param: Punktwolke
@return Eine Punktwolke pro cluster/Objekt
\end{verbatim}\label{func:euclideanClusterExtraction}

\subsubsection{SACInitialAlignment}
\chapterauthor{Alexander Link (Doku), Tammo Wübbena (Code)}
\begin{verbatim}
PointCloudRGBPtr SACInitialAlignment(std::vector<PointCloudRGBPtr> objects,
                                std::vector<PointCloudVFHS308Ptr> features,
                                PointCloudRGBPtr target)
Beschreibung: Errechnet Ausrichtung von Objekten bezüglich Zielen per sample
consensus
@param: objects Punktwolken, features als VFHSignature308, target PointCloud
@return Punktwolke
\end{verbatim}\label{func:sacinitialalignment}

\subsubsection{iterativeClosestPoint}
\chapterauthor{Alexander Link (Doku), Tammo Wübbena (Code)}
\begin{verbatim}
PointCloudRGBPtr iterativeClosestPoint(PointCloudRGBPtr input,
                                       PointCloudRGBPtr target)
Beschreibung: Errechnet Ausrichtung eines Objekts zu einem Ziel anhand eines
iterativen 
			  Closest-Point-Algorithmus.
@param: input Punktwolke, target Punktwolke
@return Punktwolke
\end{verbatim}\label{func:iterativeClosestPoint}

\subsubsection{produceColorHist}
\chapterauthor{Alexander Link (Doku), Tammo Wübbena (Code)}
\begin{verbatim}
std::vector<uint64_t> produceColorHist
(pcl::PointCloud<pcl::PointXYZRGB>::Ptr  cloud)
Beschreibung: Errechnet ein Farbhistogramm einer Punktwolke.
@param: input Punktwolke eines Objekts, dessen Farbhistogramm errechnet werden
soll
@return Farbhistogramm als floats (r,g,b)
\end{verbatim}\label{func:produceColorHist}

\subsubsection{getAllFeatures}
\chapterauthor{Alexander Link (Doku), Tammo Wübbena (Code)}
\begin{verbatim}
void getAllFeatures(std::vector<PointCloudRGBPtr> all_clusters,
                    std::vector<float> vfhs_vector,
                    std::vector<uint64_t> color_features_vector)
Beschreibung: Errechnet CVFH- und Farb-features für alle übergebenen Objekte.
@param: Punktwolken der Objekte, deren features errechnet werden sollen
\end{verbatim}\label{func:getallfeatures}

\subsubsection{getCVFHFeatures}
\chapterauthor{Alexander Link (Doku), Tammo Wübbena (Code)}
\begin{verbatim}
void getCVFHFeatures(std::vector<PointCloudRGBPtr> all_clusters,
std::vector<float> current_features_vector)
Beschreibung: Errechnet CVFH-features von Punktwolken
@param: Punktwolken der Objekte, deren CVFH-features errechnet werden sollen
@return: CVFH features (Histogramme der Winkel zwischen einer zentralen
viewpoint direction und jeder Normalen
\end{verbatim}\label{func:getcvfhfeatures}

\subsubsection{getColorFeatures}
\chapterauthor{Alexander Link (Doku), Tammo Wübbena (Code)}
\begin{verbatim}
void getColorFeatures(std::vector<PointCloudRGBPtr> all_clusters,
std::vector<uint64_t> color_features_vector)
Beschreibung: Errechnet Farbfeatures von Punktwolken
@param: Punktwolken der Objekte, deren Farbfeatures errechnet werden sollen
	    Vektor für Farbfeatures, der befüllt wird
\end{verbatim}\label{func:getcolorfeatures}

\subsubsection{getTargetByLabel}
\chapterauthor{Alexander Link (Doku), Tammo Wübbena (Code)}
\begin{verbatim}
PointCloudRGBPtr getTargetByLabel(std::string label)
Beschreibung: Lädt die PCD-Datei des Objekts, das zum übergebenen label
gehört.
@param: String label
@return Objekt-Punktwolke aus der jeweiligen PCD-Datei
\end{verbatim}\label{func:gettargetbylabel}


\subsection*{vision\_node}
\chapterauthor{Alexander Link (Doku), Beide (Code)}
\subsubsection{sub\_kinect\_callback}
\begin{verbatim}
void sub_kinect_callback(PointCloudRGBPtr kinect)
Beschreibung: Callback-Funktion für das kinect topic. Speichert die PointCloud
ab, sodass sie bereitsteht, wenn sie zur Prozessierung benötigt wird.
@param: Punktwolke des kinects
\end{verbatim}\label{func:subkinectcallback}
\subsubsection{start\_node}
\begin{verbatim}
void start_node(int argc, char **argv)
Beschreibung: Startet die Node zur Prozessierung von Punktwolken und der
Kommunikation mit anderen Nodes.
@param: argc und argv ungenutzt
\end{verbatim}\label{func:startnode}
\subsubsection{getObjects}
\begin{verbatim}
bool getObjects(	vision_suturo_msgs::objects::Request &req, 
			    vision_suturo_msgs::objects::Response &res)
Beschreibung: Service zum Extrahieren von Objekten und deren Informationen aus
einer Szene.
@param: req leere request, res Antwort mit den extrahierten Objekten nach
ObjectsInfo.msg
@return true wenn service call erfolgreich, ansonsten false
\end{verbatim}\label{func:getobjects}


\subsection*{CloudTransformer}
\chapterauthor{Alexander Link (Doku + Code)}

\subsubsection{extractAbovePlane}
\begin{verbatim}
PointCloudRGBPtr CloudTransformer::extractAbovePlane(PointCloudRGBPtr input)
Beschreibung: Findet die Hauptfläche (-> Tischplatte) und extrahiert nur die
Punkte über dieser Fläche.
@param: Punktwolke
@return Extrahierte Punktwolke (ohne die Hauptfläche)
\end{verbatim}\label{func:extractaboveplane}
\subsubsection{transform}
\begin{verbatim}
PointCloudRGBPtr CloudTransformer::transform(const PointCloudRGBPtr cloud,
std::string target_frame,
                 std::string source_frame)
Beschreibung: Transformiert eine Punktwolke in einen anderen frame.
@param: cloud Punktwolke, target_frame, source_frame
@return Transformierte Punktwolke
\end{verbatim}\label{func:transform}

\subsection*{saving}
\chapterauthor{Alexander Link (Doku), Beide (Code)}
Hilfsfunktionen zum Speichern von Punktwolken als .PCD-Dateien.

\begin{verbatim}
void savePointCloudRGBNamed(pcl::PointCloud<pcl::PointXYZRGB>::Ptr cloud,
                            std::string filename);
                            
void savePointCloudXYZ(pcl::PointCloud<pcl::PointXYZ>::Ptr cloud);

void savePointCloudXYZNamed(pcl::PointCloud<pcl::PointXYZ>::Ptr cloud,
std::string filename);

void savePointCloudNormal(pcl::PointCloud<pcl::Normal>::Ptr cloud);

void savePointCloudPointNormal(pcl::PointCloud<pcl::PointNormal>::Ptr cloud);

void savePointCloud(pcl::PointCloud<pcl::PointXYZ>::Ptr objects,
                    pcl::PointCloud<pcl::PointXYZ>::Ptr kinect,
                    pcl::PointCloud<pcl::Normal>::Ptr normals);
\end{verbatim}

\subsection*{viewer}
\chapterauthor{Alexander Link (Doku), Tammo Wübbena (Code)}
Hilfsfunktionen zur Darstellung von Punktwolken.
Enthält auch den momentan ungenutzten Visualization Marker.

\begin{verbatim}
void visualizePointCloud(pcl::PointCloud<pcl::PointXYZ> cloud);

void visualizeNormals(pcl::PointCloud<pcl::PointXYZ>::Ptr cloud,
pcl::PointCloud<pcl::Normal>::ConstPtrnormals);

visualization_msgs::Marker publishVisualizationMarker
(geometry_msgs::PointStamped point);
\end{verbatim}


\subsection{Schnittstellen}
\chapterauthor{Tammo Wübbena}

\subsubsection{Service Server /vision\_suturo/objects\_information}
\begin{verbatim}
objects.srv
@request: -
@response: ObjectsInfo msg (
							normal_features (VHFSignature308 as array)
							color_features (Color Histogram as array)
							object_amount (Number of objects in scene)
							object_information (additional information)
							object_errors (errors while extracting)
							)
\end{verbatim}
Gibt Anzahl, Features von den in einer Szene erkannten Objekten (Clustern) zurück. Die Pose wird für diesen Meilenstein noch nicht ermittelt. Der Service kann weitere Informationen und bei Problemen unterschiedliche Fehlermeldungen zurückgeben.
\\ \\
Die \textit{normal\_features} vom Typ VFHSignature308 und \textit{color\_features} (acht bins) aller Objekte werden konkateniert in Arrays festgehalten. 
Da die Länge der Features für ein Objekt immer gleich ist, kann durch die Anzahl der erkannten Objekte die Länge insgesamt ermittelt werden.

\subsubsection{Service Server /vision\_suturo/objects\_poses}
\begin{verbatim}
poses.srv
@request: string, uint8 (label of classified (recognized) object and its number
in the array given by the ObjectsInfo.msg)
@response: ObjectsInfo msg (
							poses (the pose of the requested object)
							)
\end{verbatim}
Nachdem der Service objects\_information aufgerufen wurde, werden die dadurch erzeugten Features von Planning (Weiterleitung) und Knowledge (Klassifizierung) bearbeitet. Die erkannten Objekte werden dann von Knowledge jeweils mit einem Label versehen. Der Index wird von Planning bestimmt (durch teilen des übergebenen Feature-Arrays). Danach werden die Objekte von unserem Paket bearbeitet und die Pose durch Vergleich mit den jeweiligen Meshes ermittelt. Danach werden die Posen zurück an Planning übergeben.

\subsubsection{Subscriber REAL\_KINECT\_POINTS\_FRAME}
\begin{verbatim}
@response: PointCloudRGBPtr kinect
\end{verbatim}
Gibt eine Punktwolke unseres Sichtfelds (Szene oder Scene) zurück. Dabei wird auf das Topic \textit{/kinect\_head/depth\_registered/points} horcht. Alternativ kann mit \textit{SIM\_KINECT\_POINTS\_FRAME} auf \textit{/head\_mount\_kinect/depth\_registered/points} auf Simulationsdaten gehorcht werden.
\\ \\
Die Callback-Funktion wird im loop immer wieder aufgerufen, solange die Node noch aktiv ist bzw. der roscore-server besteht.

\subsection{Programmablauf}
\chapterauthor{Tammo Wübbena}
\subsubsection{Schritt 1: Erkennen der Punktwolke}
Ein Subscriber empfängt die zu verarbeitenden Punktwolken des Kinect-Sensors und hält die wahrgenommene Szene aktuell.
\subsubsection{Schritt 2: Service-Aufruf \textit{/vision\_suturo/objects\_information}}
Jedes mal, wenn der Service \textit{/vision\_suturo/objects\_information} durch Planning aufgerufen wird, wird die Funktion \textit{findCluster} in perception.h ausgeführt. 
\subsubsection{Schritt 3: Vorbereiten auf die Extraktion} 
In \textit{findCluster} wird die Punktwolke zunächst folgendermaßen vorbearbeitet:

\begin{itemize} 
\item Ein PassThrough Filter, der den Sichtbereich begrenzt.
\item Ein VoxelGrid Filter zum Downsampling der eingehenden Punktwolke, indem für viele kleine 3-dimensionale Boxen der Mittelpunkte aller enthaltenden Punkte berechnet wird.
\item Ein MovingLeastSquares Filter, der Flächen begradigt, um Ungenauigkeiten der Daten vorzubeugen.
\item Das Entfernen von Flächen, die zu groß sind, um zum gesuchten Objekt gehören zu können. Dies passiert mehrmals, wenn es mehrere solcher Flächen im Sichtfeld gibt.
\end{itemize}
Eine Outlier-Removal wird nicht mehr benötigt, da die folgende Euclidean Cluster Extraction ''Ausreißer'', also weit von den Clustern entfernte Punkte nicht extrahiert.\\
\subsubsection{Schritt 3b: Ausweichen auf Simulationsdaten}
Sollte es zu irgendeinem Zeitpunkt beim Filtern oder der Segmentierung zu Problemen kommen, weil beispielsweise die resultierende Punktwolke leer ist, wird auf die direkten Daten des Zielobjekts aus gazebo ausgewichen, wenn möglich.
\\
\subsubsection{Schritt 4: Cluster Extraktion}
Mit \textit{EuclideanClusterExtracion} werden die nun segmentierten Objekte (alle noch in einer Punktwolke zusammengefasst) extrahiert und in einzelne Cluster gespeichert.
\\
\subsubsection{Schritt 5: Berechnen von Features und Posen}
Es werden die Normal-Features, Color-Features für die Klassifikation durch Knowledge und die Posen der einzelnen Objekte berechnet.
 
\subsubsection{Schritt 6: Befüllen der MSG}
Die einzelnen Features und Posen (letzteres aktuell noch nicht getestet) werden in der Message (ObjectsInfo.msg) gespeichert und zur weiteren Bearbeitung an Planning weitergegeben.

\subsubsection{Schritt 7: Ermitteln der Pose}
Die von Knowledge gelabelten Objekte werden mit ihren jeweiligen Meshes abgeglichen und durch SACInitialAlignment werden ihre jeweiligen Posen ermittelt.

\subsection*{Visualization-Publisher}
Der Visualization-Publisher ''publiziert'' (advertise) die extrahierten Cluster als zusammenhängende Punktwolke

\newpage
\section{Nutzungsbeschreibung}

\subsection{Installation und Ausführung von Planning}

\subsubsection{Installation}
\chapterauthor{Vanessa Hassouna}
\begin{itemize}


\item[a] Sind alle Systeme lauffähig (Vision,Knowledge und Motion), muss für Planning folgendes Repository hinzufügt werden: \url{https://github.com/menanuni/planning_suturo_1718.git}. 

\item[b] Ist die Arbeitsumgebung vollständig gebaut, wird CRAM installiert \url{http://cram-system.org/installation}.

\item[c] Nun wird die Entwicklungsumgebung installiert mit: \textbf{sudo apt-get install emacs}. Achtung: Bei der Verwendung von Ubuntu 14.04 muss der neueste Lisp 'compiler' noch hinzugefügt werden \url{https://sourceforge.net/projects/sbcl/files/sbcl/1.3.1/
} (meistens x86-64 Version). Diese Datei entpacken und im Terminal (im richtigen Ordner) den Befehl: \textbf{sh install.sh} eingeben.
\end{itemize}

\subsubsection{Ausführung}
\chapterauthor{Vanessa Hassouna}
\begin{itemize}

\item Zuerst muss im Terminal \textbf{roslisp\_repl} eingegeben werden, daraufhin startet emacs. 

\item Nun wird in der Repl (dort wo cl-user steht) ein "\textbf{,}" (ausgesprochen Komma) eingegeben.

\item Nun wird \textbf{r-l-s} eingegeben und mit \textbf{ENTER} bestätigt. Es folgt die Eingabe des Namens für ein Paket: \textbf{planning\_main\_programm} wird mit zweimaligem Befehl \textbf{ENTER} bestätigt.

\item Zum Schluss wird noch in dem cl-user Fenster \textbf{(planning-main-programm::main)} eingegeben und mit \textbf{ENTER} bestätigt (die Klammern gehören auch dazu).
\end{itemize}


\subsection{Installation und Ausführung von Motion}

\subsubsection{Installationsanleitung}
\chapterauthor{Maximilian Bertram}

\begin{itemize}
\item[a] Zunächst müssen folgende Repositories in den source-Ordner des Workspaces hinzugefügt werden:  \\
\url{https://github.com/menanuni/motion_suturo_1718} \\
\url{https://github.com/menanuni/msgs_suturo_1718} \\
\url{https://github.com/menanuni/knowledge_suturo_1718} \\

\item[b] Es muss das MoveIt Framework installiert werden, dazu im Terminal: \\
\textit{sudo apt-get update} \\
\textit{sudo apt-get install ros-indigo-moveit} \\
\textit{echo  \grqq{}source /opt/ros/indigo/setup.bash\grqq{} \textgreater\textgreater \textasciitilde{ }/.bashrc} \\

\item[c] Jetzt kann das System über \textit{catkin build} gebaut werden.
\end{itemize}

Den Stand der für Stand  des 2. Meilensteins findet sich unter folgendem Tag: \\
\url{https://github.com/menanuni/motion_suturo_1718/releases/tag/milestone2} \\

\subsubsection{Starten der Nodes}
\chapterauthor{Roman Haak}
\begin{itemize}

\item Soll das System im Kontext des echten Roboters benutzt werden, startet man zunächst über \textit{roslaunch kitchen\_model\_export knowledge\_export\_service.launch} ein Teilsystem von \textit{Knowledge}.

\item Unser Teilsystem wird dann über \textit{roslaunch motion motion\_main\_start.launch} gestartet, falls es im Kontext einer Simulation ausgeführt werden soll, bzw. über \textit{roslaunch motion motion\_main\_start\_real\_pr2.launch}, falls das Ausführen des Systems auf dem echten PR2 gewünscht ist.

\item Dann können Befehle an den Actionserver unseres Knotens über das Topic \textit{/moving/goal} abgesetzt werden. Zur Erläuterung des Actionservers siehe Dokumentation der Gruppe \textit{motion}.
\end{itemize}

\subsection{Installation und Ausführung von Vision}

\subsubsection{Installation (mit gazebo)}
\chapterauthor{Alexander Link}

\textbf{Voraussetzungen}

\begin{itemize}
\item Ubuntu 14.04
\item ROS Indigo
\item Catkin tools
\item Gazebo 2.2.x
\end{itemize}

\textbf{.bashrc}

Folgendes muss zur .bashrc (oder zshrc) hinzugefügt werden:
\begin{itemize}
\item \textit{export KINECT1=true}
\item \textit{export GAZEBO\_MODEL\_PATH= \$HOME/catkin\_ws/vision\_suturo\_1718/vision/models}
\item \textit{export GAZEBO\_RESOURCE\_PATH= \$HOME/catkin\_ws/vision\_suturo\_1718/vision/worlds}
\end{itemize}

\textbf{Setup}

\begin{itemize}
\item Gazebo installieren: \textit{sudo apt-get install ros-indigo-pr2-simulator}

\item Sollten Probleme auftreten, könnte dies durch ein Update von gazebo auf v2.2.6 gelöst werden:
\\
\textit{sudo sh -c 'echo "deb http://packages.osrfoundation.org/gazebo/ubuntu trusty main" > /etc/apt/sources.list.d/gazebo-latest.list'}
\\
\textit{wget http://packages.osrfoundation.org/gazebo.key -O - | sudo apt-key add -}
\\
\textit{sudo apt-get update}

\item Folgende Pakete in den src-Ordner eines catkin workspaces klonen:\\
        \textbf{vision\_suturo\_1718} (this package) \\
        \textbf{msgs\_suturo\_1718}
\item Erst "\textit{catkin build object\_detection}" nutzen, um die messages zu bauen.
\item Dann "\textit{catkin build}" benutzen, um alles andere zu bauen.

\end{itemize}

\textbf{Kinect benutzen}
\begin{itemize}
\item \textit{sudo apt install ros-indigo-freenect-launch freenect libfreenect-bin}
\item \textit{roslaunch freenect\_launch freenect-registered-xyzrgb.launch}
\item (Optional) \textit{roslaunch vision vision\_kinect.launch}
\end{itemize}

\textbf{Dateien speichern}

    \textit{rosrun pcl\_ros pointcloud\_to\_pcd input:=/camera/depth\_registered/point}


\subsubsection{Ausf\"uhrung}
\chapterauthor{Alexander Link}

\textbf{vision\_suturo/objects\_information}
\begin{itemize}
\item Benutzt objects.srv
\item Nimmt keine Argumente.
\item Gibt ObjectsInfo.msg zurück
\begin{itemize}
\item float32[] CVFH-Features
\item uint64[] Farb-Features
\item uint8 Objektmenge
\item string[] Weitere Objektinformationen
\item string[] Fehlermeldung (wenn vorhanden)
\end{itemize}
\end{itemize}
\textbf{vision\_suturo/objects\_poses}
\begin{itemize}
\item Benutzt poses.srv
\item Nimmt ein Label (string) und einen Index (int) entgegen.
\item Gibt einen geometry\_msgs/PoseStamped zurück.
\end{itemize}

\textbf{rosservice}

Unsere Services k\"onnen auch manuell abgerufen werden: \\

\textit{rosservice call vision\_suturo/objects\_information} \\

oder \\

\textit{rosservice call vision\_suturo/objects\_poses}

\subsection{Installation und Ausführung von Knowledge}

\subsubsection{Installation}
\chapterauthor{Alexander Haar}

\begin{itemize}
\item[1.] Java8 installieren:\\
$ https://wiki.ubuntuusers.de/Java/Installation/Oracle\_Java/Java\_8/ $\\
Wichtig JDK
\item[2.] Repo clonen und bauen
\item[3.] Die folgenden Schritte ausführen: $http://knowrob.org/installation/workspace$
\end{itemize}

\subsubsection{Ausführung}
\chapterauthor{Alexander Haar}

Um alle wichtigen Knoten zu starten muss einfach der Befehl \textit{roslaunch knowledge knowledge\_main.launch} ausgeführt werden.


\end{document}